:nl:nl:`&Process`
==================

Introduction
------------

.. nl:namelist:: Process

Fieldextra supports the simultaneous generation of multiple products.
The operations required to generate each product are defined by a set
of :nl:nl:`&Process` blocks in the namelist file. Multiple :nl:nl:`&Process` blocks are
associated with a specified product using the name of the output file
(:nl:arg:`out_file`); until a new output file name is defined, all :nl:nl:`&Process`
blocks belong to the same product definition (following the order
of :nl:nl:`&Process` blocks in the control file).
    

The :nl:nl:`&Process` block can be repeated as many times as necessary. Each
output file must be declared with one or more associated input files
(see :ref:`section 4.3.1` and :ref:`section 4.3.2`). The
data collected in a specific output file is extracted from the set of
associated input files (see :ref:`section 4.3.3`). Operations
on extracted fields and computation of derived fields are applied before
writing the output on disk (see :ref:`section 4.3.4`
and :ref:`section 4.3.5`).

The typical sequence of :nl:nl:`&Process` blocks for a specific :nl:arg:`out_file` is: ::

    &Process  in_file= ...                                    | This can be repeated when
              out_file= ...    /                              | multiple input contribute
    &Process in_field= ...     /     (1 or more occurences)   | to the same output
    &Process tmp1_field= ...   /     (0 or more occurences)
    &Process tmp2_field= ...   /     (0 or more occurences)
    &Process tmp3_field= ...   /     (0 or more occurences)
    &Process tmp4_field= ...   /     (0 or more occurences)
    &Process tmp5_field= ...   /     (0 or more occurences)
    &Process out_field= ...    /     (0 or more occurences)

This sequence can be repeated as many times as necessary. The same input
and output files will be used until new file names are explicitely
specified; all other variables are reset to their default value before
reading the next :nl:nl:`&Process` block.

    
A case with two input files contributing to a single output is presented
below: ::

    &Process  in_file  = file_1       < Specification of 1st input file
              out_file = ...          < Specification of output file
    
    &Process in_field = field_1   /   < Specification of fields to
    &Process in_field = field_2   /     extract from first input file
    &Process in_field = field_3   / 

    &Process  in_file  = file_2       < Specification of 2nd input file
              out_file = ...          < Specification of output file (repeated)
    
    &Process in_field = field_4   /   < Specification of fields to
    &Process in_field = field_5   /     extract from second input file

    &Process tmp1_field = field_1 /   < First iteration. 
    &Process tmp1_field = field_5 /     Specification of fields to transfer or generate;
    &Process tmp1_field = field_6 /     field_1... field_5 are at disposal.

    &Process out_field = field_1  /   < Last iteration. 
    &Process out_field = field_5  /     Specification of fields to transfer or generate;
    &Process out_field = field_6  /     field_1, field_5, and field_6 are at disposal.
    &Process out_field = field_7  /     All fields specified here are written in output.
    
    
When :nl:arg:`&RunSpecification strict_nl_parsing` is true (which is the default), fieldextra requires that
all :nl:nl:`&Process` definitions associated to each single output follow the order given
in the previous example: first the input/output characteristics are specified
together with the fields to extract, for each contributing input/output pairs,
and only then are the processing iterations specified, in the order of the
iterations that are made.
    

As explained in detail further on, it is possible to use some shortcuts to
write more compact namelists. In particular it is possible to use an implicit
time loop in the definition of :nl:arg:`in_file`, and to suppress the out_field group
when all extracted fields have to be available in :nl:arg:`out_file`.

.. _section 4.3.1:
    
Input files
-----------

Regular input (i.e. input not declared as INCORE) is implemented for
the following formats:

1.  GRIB edition 1 or 2, including files mixing the type of GRIB 
    records. The DWD GRIB library is used to decode GRIB 1 records,
    and ECMWF ecCodes is used to decode GRIB 2 records. 
    Non-standard GRIB 1 modifications used in the COSMO consortia
    are supported.

2.  NetCDF, following the CF 1.6 conventions, with the following 
    limitations:

    *   | at most one time dimension
        | (axis='T', or standard_name='time', or name='time') 
    *   | at least one X and one Y dimensions
        | (X: axis='X', or standard_name in {'grid_longitude','longitude'...}
        | or units in {'degrees_east','degree_east' ...};
        | Y: axis='Y', or standard_name in {'grid_latitude','latitude'...}
        | or units in {'degrees_north','degree_north' ...})
    *   ignore variables not associated with exactly one X and one
        Y dimensions
    *   ignore variables not associated with a unique valid field
        short name 

    The dimensions recognized by fieldextra are X, Y, Z, T and the EPS
    member dimension.

    The NetCDF data is processed as a set of 2d horizontal slices,
    i.e. variables with fixed dimensions except X and Y, sorted
    according to: time dimension, variables identificator, any
    other dimensions (from outer loop to inner loop). 

    Required field attributes which are missing in the imported
    NetCDF file can be set in the namelist header, in particular
    by using :nl:arg:`&NetCDFImport dim_default_attribute` and :nl:arg:`&NetCDFImport var_default_attribute`.

    The rules to define the short name associated with a NetCDF
    variable are: use the :nl:nl:`&NetCDFImport` translation table when
    available (varname_translation), otherwise use the short name
    attribute, otherwise use the variable name itself.
    Once a field short has been defined, the NetCDF variable name
    is used as field tag; by setting in_duplicate_field=.true.,
    this supports working with multiple instances of the same field
    in the input file.

3.  BLK_TABLE release 1.8 or higher, with a verbosity level of 2
    or higher. The following limitations apply:

    *   do not support options epsinfo, pdfinfo, and depreciated;
    *   shifted grid is not coded in BLK_TABLE, it is assumed that
        all fields are defined on mass points;
    *   reference system is not coded in BLK_TABLE, it is assumed 
        that all vector fields are defined with respect to geog. 
        reference system.

When a choice exists between NetCDF and GRIB2 for the format of the
input files, one should consider the fact that GRIB2 supports more
automated meta-information checks; in this sense, GRIB2 is a better
choice when the consistency of the processing chain has a high 
priority. 


The following assumptions are made about the content of regular
input files:

1.  all fields are valid for the same date; 
2.  each multi-level field is defined for a unique level type;
3.  when file names only differ by a suffix {.*,c,z,p,s,o,n},
    the associated validation dates are identical;
4.  all fields belong to the same EPS member (if relevant).

If some of the conditions 1 to 4 are not fulfilled, error messages
of type 'Bad estimation of internal storage...' may be issued;
explicitly setting the internal storage size will solve this issue
(see in_size_vdate and in_size_field).

Input files are processed twice: first to collect data for pseudo-output
(INCORE, INSPECT), then to collect data for other output.

Processing order of input files is defined by the following rules:

Case A : :nl:arg:`in_read_order` is not set
    1.  files with the same time stamp are grouped together
        (a file defined both with and without a time stamp is 
        recognized as a unique input file and sorting is based
        on the time stamp value);
    2.  within a group defined by rule (1), files are sorted
        according to file path in increasing alphanumeric order;
    3.  groups are sorted according to their common time stamp:
        group with empty time stamp is put first,
        groups with absolute time stamp are put next,
        groups with relative time stamp are put last;
        within each of these classes, the order of the groups
        follows increasing alphanumeric order of associated
        time stamp;
    4.  re-order some special files
        (pure input: :nl:arg:`in_file` not declared as :nl:arg:`out_file`)
        
        1.  move non pure input at the end of the group containing
            the last contributing pure input, respecting otherwise
            the order defined by (1-2);
        2.  files marked with in_read_first=.true. are moved
            at the very start of the list;

    5.  INCORE pseudo-file is accessed on-demand, and, for each
        output, before the first contributing standard input file.

Case B : :nl:arg:`in_read_order` is set
    in this case, :nl:arg:`in_read_order` must be set for all 
    input files, and in_sort_first is not allowed

    1.  files are processed following increasing values of
        :nl:arg:`in_read_order`;
    2.  within a group of files having the same value of
        :nl:arg:`in_read_order`, rules 1-3 of case A applies; 
    3.  INCORE pseudo-file is accessed on-demand, and, for each
        output, before the first contributing standard input file.

These rules support the following features:

*   concurrent computation of model and postprocessing (rules 1 & 3):

    as long as alphanumeric order of time stamps corresponds to
    increasing dates, which is the case for the naming conventions
    of COSMO, fieldextra expects the input files in the same order
    as they are produced by the COSMO model.
*   just on time mode (rules 1 & 3):

    data is stored and processed independently for each validation
    date, but order of data in output file respects natural order.
*   iterative product generation within the same run (rule 4.1):

    a first set of rules can be applied to generate an temporary
    GRIB file on disk, which is read and processed by another set
    of rules later on but within the same run (only a single
    intermediate step is supported!); this can be used e.g. to 
    compute a set of products on different subdomains, derived
    from a single set of expensive computations.
*   minimization of memory footprint for problems where the same
    INCORE data is dispatched in multiple output:
    on-demand access of INCORE means that the memory allocation
    of each receiving output is done at the last possible moment.

Policy on missing input file is controlled by the values of
wait_time, mx_wait_time, and clone_missing_input (one of
skip [default], clone, wait and exit, wait and clone).

.. nl:argument:: in_file
    :format: <input file name> 

    The name __INCORE\__ is reserved

It is often necessary to apply the same operations to a set of
files, each file of the set being characterized by a unique
validation date or representing a different EPS member.
Instead of repeating the :nl:nl:`&Process` group, a mechanism supports
the definition of an implicit loop on the input file name.
This mechanism relies on the file name conventions defined for
the COSMO model.

Note that all files belonging to the same set must have the
same type.

time loop
^^^^^^^^^

If input files associated with different dates are processed,
a generic name using one of the following keyword may be used:

*   | (lead time loop)
    | ``<hh>``, ``<hhh>``, ``<ddhh>``, ``<ddhhmmss>``,
*   | (validation date loop)
    | ``<yyyy:start_year>``, ``<yyyymmddhh:start_date>``,
    | ``<yyyymmddhhmm:start_date>``,
*   | (reference date loop)
    | ``<reference_yyyy:start_year>``, ``<reference_yyyymmddhh:start_date>``,
    | ``<reference_yyyymmddhhmm:start_date>``,
*   | (time step loop)
    | ``<nnnnnnnn>`` 

These keywords are primary used to group similar input files,
which supports more compact namelist and help optimize some
internal operations. Usually, besides the file name, no special
meaning is associated with the keyword. In particular, 
keywords which are only differentiated through the presence
of the 'reference\_' string can be used indifferently.

In some cases, however, the associated time stamp is interpreted
as meta-information for the input files records, according to
what is noted in the previous table. Currently, this is the
case when: (1) clone_missing_input is true, to replace missing 
files.

Only a single time loop keyword is supported within a generic name,
any additional keyword remains untransformed.

The list of input files is defined by specifying a list of
times, either explicitly ...

.. nl:argument:: tlist
    
    :format: <time>,<time>,...

    -1 means no time list

or implicitly ...

.. nl:argument:: tstart
    
    :format: <start time>

.. nl:argument:: tstop
    
    :format: <stop time>

.. nl:argument:: tincr
    :default: 1

    :format: <increment>

The list of input files is created by replacing the time keyword
by each value of the list in turn, according to the following 
rules:
    
* ``<hh>`` : time stamp is a two digits hour - specified times are
  interpreted as hours 
* ``<hhh>`` : time stamp is a three digits hour - specified times are
  interpreted as hours 
* ``<ddhh>`` : time stamp is a two digits day followed by a two 
  digits hour - specified times are interpreted as hours
* ``<ddhhmmss>`` : time stamp is a two digits day followed by two
  digits hour, minute and second - specified times are
  interpreted as seconds (but must represent full minutes)
* ``<yyyy:start_year>``, ``<reference_yyyy:start_year>`` :
  time stamp is a 4 digits year - specified times are
  interpreted as years 
* ``<yyyymmddhh:start_date>``, ``<reference_yyyymmddhh:start_date>`` :
  time stamp is a 4 digits year followed by two digits month,
  day and hour - specified times are interpreted as hours and
  added to start_date (which must use the same representation)
* ``<yyyymmddhhmm:start_date>``, ``<reference_yyyymmddhhmm:start_date>`` :
  time stamp is a 4 digits year followed by two digits month,
  day, hour and minute - specified times are interpreted as
  minutes and added to start_date (which must use the same
  representation)
* ``<nnnnnnnn>`` : time stamp is expressed in number of time steps

Temporal granularity of input fields down to one minute is 
supported.

eps loop
^^^^^^^^

If input files associated with different EPS members are processed,
a generic name using one of the following keyword may be used:

* | (EPS member identity)
  | ``<mm>``, ``<mmm>``

As for the previous discussion about the time loop, the same remarks
about the meaning of these keywords apply.

Only a single eps loop keyword is supported within a generic name,
any additional keyword remains untransformed.

The list of input files is defined by specifying a list of
members, either explicitely ...

.. nl:argument:: epslist
    
    :format: <eps member>,<eps member>,...

    -1 means no eps list

or implicitly ...

.. nl:argument:: epsstart
    
    :format: <start eps member>

.. nl:argument:: epsstop
    
    :format: <stop eps member>

.. nl:argument:: epsincr
    :default: 1


    :format: <increment eps member>


The list of input files is created by replacing the keyword by a
two (``<mm>``) or three (``<mmm>``) digits string, for each member of the
epslist in turn.

other characteristics
^^^^^^^^^^^^^^^^^^^^^

.. nl:argument:: in_type
    :default: 'PLAIN'    


    :format: <input file type>   


    In addition to plain input files, this program may also access pseudo
    files used as intermediate storage. In this case the value of in_type
    has to be set. 
    Possible values of in_type are:

    INCORE  : 
            pseudo input type to support direct access to fields
            stored in INCORE storage (see 4.3.2 below). The
            parameter :nl:arg:`in_file` must not be set in this case.
    PLAIN   : 
            plain input file; the format currently supported are
            WMO GRIB, edition 1 and 2, NetCDF (CF convention), and
            BLK_TABLE release 1.8 or higher.


    It may be necessary to change the programatic sorting of input files,
    as described at the beginning of this section, in order to reduce the
    memory footprint of some applications (see 2.5). This is possible by
    using one of the following mutually exclusive variables:

.. nl:argument:: in_read_first
    :default: .false.

    :type: logical

    forces the associated input file to be processed just
    after INCORE pseudo-input. Not compatible with input also declared as
    output in another procesing block.

.. nl:argument:: in_read_order


    :format: <strictly positive integer>

    default undefined
    
    the input files will be processed following increasing
    order of in_read_order, but, in any cases, after INCORE pseudo-input;
    within a group having the same in_read_order value, the order is the
    programatic order. When used, each standard input file must be 
    associated with a value of in_read_order. Values must be set to 
    ensure that any input also declared as output in another procesing
    block is processed after all 'pure' input files.

.. nl:argument:: in_dictionary
    :type: string

    supersede value of default_dictionary
     
    Re-define the active dictionary used to decode and process the fields
    extracted from the associated input file. 
   
    When available, it is checked that the values of MODEL_TYPE and of
    PRODUCT_CATEGORY are compatible with the characteristics of the decoded
    record.
   
    The special value __AUTO\__ is used to defer the choice of the dictionary
    till a record is decoded, independently for each record. The dictionary
    is selected on the basis of the decoded model type and product category,
    in the list of dictionaries specified in :nl:nl:`&GlobalResource`
    is associated with a model type (keyword MODEL_TYPE) and, optionaly, with
    a list of product category (keyword PRODUCT_CATEGORY), and these 
    information are used to select the active dictionary:
    first a dictionary with an exact match is looked for. If not found, a
    dictionary with PRODUCT_CATEGORY set to blank and a matching model type
    is looked for. If not found, a dictionary with MODEL_TYPE set to blank
    is looked for.
    The special value __AUTO\__ mainly supports the use of fieldextra as a
    computation engine in a wrapper script. It can only be used in conjunction
    with in_field='__ALL\__'.
   
    It is not allowed to collect fields associated with different dictionaries 
    in the same output (see also 'ignore_model_identity').

.. nl:argument:: default_product_category
    :type: string

    default product category for associated input

    The default product category is used to supplement missing or unrecognized
    information in the input records of the associated file.
    
    See the definition of :nl:arg:`&GlobalSettings default_product_category` for the list of
    supported values and a discussion on the use of this parameter.

.. _section 4.3.2:

Output files
------------

For each output file, the set of contributing input is built using
the definitions found in the :nl:nl:`&Process` block(s) where this output is
mentioned; the set of input fields available for the production of
this ouput is limited to the contributing input files.

The set of contributing input depends on the usage of time and eps
keywords in the definition of the input and the output, on the
usage of :nl:arg:`tlag` and on the usage of :nl:arg:`out_group_eps`, as explained below.

Normally, the production of any specified output is done only once all
contributing input files have been processed, or, at the latest, at the
end of the program. However, to optimize the memory footprint, the
program will try in some cases to generate the output file 'on the fly',
by appending new data to the output file each time a contributing input 
has been processed. This is the so-called 'just on time' mode. 

**'just on time mode'**
    *   by default, automatically set, independently for each output
        (but :nl:arg:`out_type_justontime` allows explicit control)
    *   this mode controls the way multiple validation times are
        processed: either in one pass at the end of the program,
        or each time the data for a specific validation time have
        been collected. In this latter case the memory footprint
        of the program may be drastically reduced.
    *   The following conditions must be satisfied for this mode
        to be active:

        *   output supports just on time mode (supports the use 
            of :nl:arg:`out_type_justontime`, see output characteristics in
            :ref:`chapter 7`)
        *   all time operators are set to identity
        *   no computed fields requiring parents on multiple time levels
        *   no use of out_postproc_module (could contain time operator)
        *   no use of time stamp in :nl:arg:`out_file` definition
        *   all contributing input files, with the exception of INCORE,
            are declared with the same non trivial time keyword and are
            associated with the same time list.

For real time applications, such as the post-processing of the direct
model output in a production suite, fieldextra can be configured to
generate partial output concurrently to the further processing of new
input files. This can be achieved by using the so called

**'snapshot mode'**
    the program runs concurrently to the production of input files 
    and wait for input files as necessary (controlled by :nl:arg:`&RunSpecification wait_time` and 
    :nl:arg:`&RunSpecification mx_wait_time`). The production of intermediate (incomplete) output may
    be triggered by the use of out_type_snapshot.

To guarantee the integrity of output files during production, the
postfix '.iolck' is appended to the output file name during the io
operations.

When output files are iteratively produced, it is possible to flag
all output files which are not yet complete. This is achieved by
setting :nl:arg:`&RunSpecification out_noready_postfix`.

 
.. nl:argument:: out_file

    :format: <output file name> 

    The names __INCORE\__ and __INSPECT\__ are reserved

time and eps loop
^^^^^^^^^^^^^^^^^

As for the definition of :nl:arg:`in_file`, it is possible to use a generic
name for :nl:arg:`out_file`, using one of the following keyword:

time related, refer to :nl:arg:`tstart` or :nl:arg:`tlist` :
    ``<hh>`` [1]_, ``<hhh>`` [1]_, ``<ddhh>`` [1]_, ``<ddhhmmss>`` [1]_,
    ``<yyyy:start_year>``, ``<reference_yyyy:start_year>``
    ``<yyyymmddhh:start_date>``, ``<reference_yyyymmddhh:start_date>``
    ``<yyyymmddhhmm:start_date>``, ``<reference_yyyymmddhhmm:start_date>``
    ``<nnnnnnnn>`` [1]_
eps related, refer to :nl:arg:`epsstart` or :nl:arg:`epslist` :
    ``<mm>``, ``<mmm>``

.. [1] 
    These time related keywords support a constant shift
    in their definition. This shift can be both positive or negative and
    is expressed as in the following example: ``<ddhh+shift>``, ``<ddhh-shift>``.
    This functionality supports in particular the construction of lagged
    ensemble.

As for :nl:arg:`in_file`, the list of output files is constructed by replacing 
the eps or time keyword with each element of the specified eps or time
list in turn (as specified by :nl:arg:`epslist`, :nl:arg:`epsstart`... or :nl:arg:`tlist`, :nl:arg:`tstart`...).
In case of time keyword, the units for the values of the time list
are defined according to the :nl:arg:`in_file` time keyword, if present; 
otherwise the :nl:arg:`out_file` time keyword defines these units.

These keywords are primary used to group similar output files created
with the same operations, which supports more compact namelist and
help optimize some internal operations. However, this feature also
supports some additional functionalities, such as distributing the
input file records in multiple output (see :nl:arg:`out_shuffle_time` and :nl:arg:`out_shuffle_eps`).
    
When the set of contributing input files has been defined by an
implicit time loop (generic name), it is possible to restrict the
set of output files, when :nl:arg:`out_file` is defined by a generic name,
and the set of active dates collected in each output, by defining
a new time list :

.. nl:argument:: out_tstart

    :format: <start time for filtering output>  

.. nl:argument:: out_tstop

    :format: <stop time for filtering output>  

.. nl:argument:: out_tincr

    :format: <increment for filtering output> 

The effects of this time list are:

1.  Restrict the set of produced output files to those having a
    time stamp part of the list (files without time stamp are not
    filtered out);
2.  For each output, restrict the set of collected dates to the ones
    present in the contributing input files with a matching time stamp
    (files without time stamp are not filtered out). Note that any
    operator defined for the production of the output (e.g. :nl:arg:`toper` )
    has access to all time levels, the filter acting only after the
    generation of the output fields.

The set of input fields available for the production of some output
is restricted by the set of input files associated with the concerned
output. When the output file contains a time or eps keyword in its
definition, the default rule is to associate the files with the same
time or eps keyword.
This default behaviour does not support the case where multiple time
levels or multiple EPS members are required to compute the output
fields (e.g. use of :nl:arg:`toper`, computation of EPS derived fields). For 
this reason the default rule can be changed by the usage of :nl:arg:`tlag`
or :nl:arg:`out_group_eps`, as explained below.

.. nl:argument:: tlag

    :format: <start time>[,<stop time>[,<increment>]]


    The variable :nl:arg:`tlag` defines a set of times through an implicit time loop;
    the following conditions must be respected:

    *  start time <= 0, start time <= stop time, increment > 0
    *  0 must be part of the set of time
   
    The set of times defined through :nl:arg:`tlag` specifies the set of input files
    contributing to each output, expressed as time differences relatively
    to the time stamp defined for each concerned output:

    Let's T be the time associated with the current output.
    Let's tlag(1), tlag(2), ... be the list of times derived from tlag.
    The contributing input files are those built with the time stamps
    T+tlag(1), T+tlag(2), ... , T+tlag(n)  
    
    All associated time levels (and only these) are available for any
    operator; however, only the time level associated with T is written
    in the concerned output file.
   
    Two optional syntax are also supported:

    * When :nl:arg:`tlag` is only defined through <start time>, the other components
      take the values ::     

        <stop time> = 0
        <increment> = -1 * <start time> 

    * When <increment> is missing, but <stop time> is present, the increment
      takes the value ::          
        
        <increment> = 1

      Furthermore, in this case (and only in this case) contributing
      input files associated with T+tlag(i) may be missing without
      triggering an exception. This syntax should be used when working
      with irregular list of times.
   
    The variable tlag is mostly used in relation with a temporal operator
    (:nl:arg:`toper`), when the fields in each output file are the result of some
    temporal transformation, but a unique output file is desired for each
    time level; in this situation, it is important to make sure that the
    set of contributing times specified by :nl:arg:`tlag` is compatible with the
    set of times levels required by :nl:arg:`toper`, as specified in :nl:arg:`toper` argument.
   
    Note that :nl:arg:`tlag` can only be set when both :nl:arg:`in_file` and :nl:arg:`out_file` are
    defined as generic name, with a time keyword. 

.. nl:argument:: out_group_eps
    :default: .FALSE.

    :format: <.TRUE. or .FALSE.>


    Controls the interpretation of :nl:nl:`&Process` blocks using eps keywords
    in the definition of outfile. By default, each output defined in 
    such a :nl:nl:`&Process` block is only associated with the input having 
    the same EPS identity (or with any input without eps keyword); this
    prohibits the usage of operators requiring access to all EPS
    members.
    By setting out_group_eps to true, a unique common internal storage
    is used for all outfile sharing the same EPS keyword; this removes
    the previously stated limitation on the supported operators.
   
    A typical application of this feature is to implement a transformation
    of a set of EPS members, with each member in a unique file, where the
    transformation uses EPS reduction operators (e.g. mean, deviation).
   
    When setting :nl:arg:`in_size_field`, consider all fields contributing to the
    common internal storage (this means in particular multiplying the
    number of extracted fields by the number of distinct EPS members in
    the contributing input).
   
    When a problem is detected in the production of one of the output,
    the production of all other associated output is interrupted.

The following operators are used to split an input file into multiple output,
grouping fields with the same characteristics in the same output.

.. nl:argument:: out_shuffle_time
    :default: .FALSE.

    :format: <.TRUE. or .FALSE.>

    By setting :nl:arg:`out_shuffle_time` to true, the fields are dispatched in
    different output on the basis of their reference date, validation
    date or lead time. A time keyword must be used in the definition
    of :nl:arg:`out_file`, and a list of times must be specified. The way the
    fields are grouped depends on the time keyword:

    *   when using <yyyy:start_year>, fields with the same year
        of validity are grouped together;
    *   when using one of <yyyymmddhh:start_date> or 
        <yyyymmddhhmm:start_date>, fields with the same validation
        date are grouped together;
    *   when using <reference_yyyy:start_year>, fields with the
        same year of reference are grouped together;
    *   when using one of <reference_yyyymmddhh:start_date> or 
        <reference_yyyymmddhhmm:start_date>, fields with the same
        reference date are grouped together;
    *   when using <hh>, <hhh>, <ddhh> or <ddhhmmss>, fields with
        the same lead time are grouped together.

    Constant fields are dispatched in all output.

.. nl:argument:: out_shuffle_eps
    :default: .FALSE.

    :format: <.TRUE. or .FALSE.>

    By setting :nl:arg:`out_shuffle_eps` to true, the fields are distributed 
    in different output on the basis of their EPS member values. An
    EPS member keyword must be used in the definition of :nl:arg:`out_file`,
    and a list of EPS members must be specified. EPS fields not
    associated with a specific EPS member value (e.g. mean), and 
    non EPS fields are dispatched in all output.


in field manipulation
^^^^^^^^^^^^^^^^^^^^^

.. nl:argument:: in_duplicate_field
    :default: .FALSE.

    :format: <.TRUE. or .FALSE.>

    By default a single instance of each 2d field is allowed in the
    :nl:arg:`in_field` iteration (a 2d field is characterized by its short name,
    level, eps identity...). This can be changed by setting
    :nl:arg:`in_duplicate_field` to true.

    When :nl:arg:`in_duplicate_field` is true, tags in the :nl:arg:`in_field` iteration
    may be inherited from the input file or may be programatically set,
    as explained in :nl:arg:`tag`.
    This is required to discriminate between multiple instances of the same field.

.. nl:argument:: out_autotag
    :default: .FALSE.

    :format: <.TRUE. or .FALSE.>

    Tags are programatically set when :nl:arg:`out_autotag` is true. Each input
    field not associated with a user defined tag or an inherited tag is
    tagged with '__autotag_fn\_#__', # being the order of occurence
    of the corresponding field in the input stream and fn the field
    short name.

The association between contributing input records and inherited 
or automatic tags is documented in 'fieldextra.diagnostic' when
:nl:arg:`&RunSpecification additional_diagnostic` is set to true.


A specific instance of a field can be selected in the :nl:arg:`in_field`
iteration by setting :nl:arg:`use_tag` to the value of the inherited or 
automatic tag.

Note that it is necessary to explicitely set the number of
contributing fields when setting :nl:arg:`in_duplicate_field`, by
using :nl:arg:`in_size_field`.

.. nl:argument:: in_size_field
    :default: 0,0

    :format: [<size1>,] <size2>

.. nl:argument:: in_size_vdate
    :default: 0

    :format: <size>


The size of the internal storage is automatically evaluated on the
assumption mentioned at the beginning of the section :ref:`section 4.3.1`. If some of these assumptions are not respected, it may
be necessary to manually set these values. These parameters can also
be used to minimize the memory footprint when the program is unable
to precisely estimate the required storage (:nl:val:`selection_mode=EXCLUDE`,
:nl:val:`selection_mode=INCLUDE_ALL`, :nl:val:`in_field=__ALL__`). 

The value of :nl:arg:`in_size_field` specifies the field dimension of the 
storage size (each 2d field takes one slot), and the value of
:nl:arg:`in_size_vdate`  specifies the validation date dimension of the
storage size.

When two :nl:arg:`in_size_field` values are specified, the first value refers
to the size of the storage required for the first contributing input
file, and the second value refers to the total required size; this is
used to support memory optimization :nl:arg:`in_read_first`.

These values must not be exact, but should define an upper bound
of the required storage size.

controling load balance
^^^^^^^^^^^^^^^^^^^^^^^

.. nl:argument:: out_cost
    :default: 1

    :format: <positive integer>


    To improve the load balance of the code running in parallel mode, a
    relative cost can be given to each output. The relative cost is
    expressed as a strictly positive integer, larger values meaning more
    expensive products. A cost of 1 is defined per default. 
   
    See :ref:`section 2.6` for more details.


re-gridding
^^^^^^^^^^^

As a general rule, all fields collected for a specific output must be defined
on a common grid. Exceptions are for fields defined on a staggered grid, and
when the target grid is a subset of the grid associated with the input field.

It is possible to re-grid any input fields from its native grid to a common
base grid; one has to specify a re-gridding algorithm, and each input field
flagged with :nl:val:`regrid=.true.` (or all fields when  :nl:arg:`in_regrid_all` is true) will
be interpolated on the common base grid just after being extracted from the
input file (see detailed description below).

The following possibilities are available when working with fields defined
on staggered grid:

1.  The user re-grid the fields defined on the staggered grid by using the
    :nl:arg:`in_regrid_method` operator described below. This method is global,
    meaning that de-staggering is computed once when collecting the 
    concerned fields, the de-staggered field being dispatch in all
    output using this field.
2.  The user re-grid the fields defined on the staggered grid by using the
    horizontal operator :nl:val:`hoper='destagger'` (see :nl:arg:`hoper`). This
    operator can be used in any processing iteration; with the use of tags,
    this operator support having both staggered and un-staggered version
    of the same field (or of derived fields) in the same output.
3.  The user specifies the model base grid with in_regrid_target='tag', 
    without defining a re-gridding algorithm; the program will keep track 
    of the field native grid when a supported staggering is detected, but
    will not re-grid these fields (this option is only possible when the
    chosen output type supports staggered grids, e.g. GRIB1 or BLK_TABLE,
    or when the staggered fields are only used temporarily to compute some
    derived quantity).
4.  The user specifies the model base grid with :nl:val:`in_regrid_target='__AUTO__'`, 
    without defining a re-gridding algorithm; this is in principle equivalent
    to 3., the model base grid being in this case defined as the grid
    associated with the first collected field. This switch is mainly used
    to support wrapper scripts and should be used with caution (some operators
    require an absolute positioning of the parent fields with respect to the
    model base grid, and may produce erroneous results when defining some
    arbitrary base grid).

The following possibilities are available when the target grid is a subset
of the grid associated with the input field:

1.  The user re-grid the fields defined on the original grid by using the
    :nl:arg:`in_regrid_method` operator described below. 
2.  The user specifies the common target grid with :nl:val:`in_regrid_target='tag'`,
    without defining a re-gridding algorithm (tag refers to some INCORE
    field); all input fields will be cropped to match :nl:arg:`in_regrid_target`.

This latter mechanism (point 2 above) can also be used when working with
fields which are both staggered and defined on a larger domain than the
target domain. However, in this case, the de-staggering is not always 
unambiguously defined; the convention is then to use a positive staggering.

Further opportunity to re-grid all collected fields associated with a specific
output is provided by the operator  :nl:arg:`out_regrid_target`  (see description later on).

When both  :nl:arg:`in_regrid_target`  and  :nl:arg:`in_regrid_method`  are defined, field interpolation 
will take place:

.. nl:argument:: in_regrid_target

    :format: <target grid for re-gridding, or common base grid>

    This can be one of

    * ``"__AUTO\__"``                                     
    * ``"<tag>"``                                          
    * ``"<grid>,<xmin>,<ymin>,<xmax>,<ymax>,<dx>,<dy>"``               
    * ``"<grid>,<xmin>,<ymin>,<xmax>,<ymax>,<dx>,<dy>,<pollon>,<pollat>"``

    with:
            
    ``<tag>``: 
      tag of incore field used to define output grid
    ``<grid>``: 
      target coordinates (one of "swiss","boaga-west","boaga-east", "geolatlon","rotlatlon")
    ``<xmin>``, ``<ymin>``: 
      coord. of start point (south-west)
    ``<xmax>``, ``<ymax>``: 
      coord. of end point (north-east)
    ``<dx>``, ``<dy>``: 
      increments in each direction (> 0)
    
    and for grid="rotlatlon"

    ``<pollat>``, ``<pollon>``: 
      latitude and longitude of rotated N pole

    coordinates translate as:
    
    * x refers to West-East direction
    * y refers to South-North direction
    
    where all values are integers expressed in units of
    
    * [m] for swiss, boaga-west, boaga-east
    * [10**-6 degree] for geolatlon, rotlatlon
  
    allowed range for x and y are defined in the following table

    .. flat-table:: 
      :header-rows: 1

      * - target coordinates
        - x (longitude) range
        - y (latitude) range

      * - geolation, rotlation
        - [0,360000000[
        - ]-90000000,90000000[

      * - swiss
        - ]100000,1100000[
        - ]-200000,500000[

      * - boaga-west
        - ]1000000,2000000[
        - ]4000000,5500000[

      * - boaga-east
        - ]2000000,3000000[
        - ]4000000,5500000[

.. nl:argument:: in_regrid_method

    :format: <re-gridding algorithm(s)>
    
    applied just after data extraction

    This can be one of

    * | ``"default"``
      | refer to the definition of the regridding method in the active :nl:nl:`&ModelSpecification` (see :nl:arg:`&ModelSpecification regrid_method`)
    * ``<regrid_method>``  
    * ``<f1>,...:<regrid_method>[,<f2>...:<regrid_method> ...]``  (only for regrid_method in &ModelSpecification)

    ``<regrid_method>`` can have one of the following forms:
    
    * ``"from_unstructured_grid,<method>[,<argument>]"``
    * ``"<method>,[<neighbourhood>,<neighbourhood_radius>[,<exp_scale>]]"`` 

    There are multiple methods available, depending on whether we want to re-grid from unstructured or regular grid.


    ==================== ==========================================
    ``<method>``          Description
    ==================== ==========================================
    **Re-gridding from unstructured grid**
    ---------------------------------------------------------------
    ``"rbf"``             radial basis functions;
                          supports one optional real argument,
                          defining the scale factor
  
                          'expert guess' table for the scale factor:
  
                          ======== ====== =====
                          ======== ====== =====
                          R02B06   40km   0.5
                          R02B07   20km   0.35
                          R02B08          0.2
                          R02B09          0.05
                          ======== ====== =====
      ``"byc"``           barycentric interpolation, meaning
                          * continuous
                          * interpolates the data exactly
                          * no over- and undershoots
      ``"nnb"``           nearest neighbour interpolation
    **Re-gridding from unstructured grid**
    ---------------------------------------------------------------
    ``"bilinear"``       bilinear interpolation, using summits of
                         enclosing source mesh element as control
                         points
                         (no other arguments required)
    ``"next_neighbour"`` value of next neighbour on source grid
                         (neighbourhood required)
      ``"average"``      arithmetic mean of values at neighbouring
                         source points
                         (neighbourhood required)
      ``"dist_average"`` inverse distance weighted mean of values
                         at neighbouring source points
                         (neighbourhood required)
      ``"log_average"``  geometric mean of values at neighbouring
                         source points
                         (neighbourhood required)
      ``"exp_weight"``   weighted mean of values at neighbouring
                         source points; weights given by
                         exp(-distance(source,target)/exp_scale)
                         (neighbourhood & exp_scale required)
      ``"linear_fit"``   approximation by bivariate polynomial of 
                         degree 1
                         (neighbourhood required)
      ``"dominant"``     histogram-based determination of the
                         dominant neighbouring value; only values
                         in the interval (0,50] are considered and 
                         a bar width of 1 is chosen to set up the
                         histogram)
                         (neighbourhood required)
    ==================== ==========================================

    The remaining fill-ins translate as follows

    ``<neighbourhood>``
      how to associate source and target points (one of ``"square"``, ``"circle"``)
    ``<neighbourhood_radius>``
      radius of neighbourhood, in units of target mesh size
    ``<exp_scale>`` 
      factor defining exponential weight

    For :nl:arg:`&ModelSpecification regrid_method`, re-gridding method applicability can be restricted to a set of fields,
    as specified by an optional list of one or more comma separated field
    short names (``<f1>``, ``<f2>``...); the default re-gridding method is specified 
    by omitting this list or by using the special short name ``"__ALL__"``.
                    

.. nl:argument:: in_regrid_all
    :default: false
    :type: logical

    switch to re-grid all input fields

    When :nl:arg:`in_regrid_all` is true, all input fields are re-gridded; otherwise, only the
    fields marked with :nl:val:`regrid=.true.` are re-gridded (see the description of regrid
    later on).
            
   
Both re-gridding from regular grids and from unstructured grids are supported.
The definition of the target grid is done with :nl:arg:`in_regrid_target`.
The choice of the re-gridding method is set with :nl:arg:`in_regrid_method`; multiple
methods may be defined, including a default method and a set of field specific
methods (using the ``"default"`` mechanism).
When re-gridding from unstructured grid, the description of the source grid
must be made available through :nl:nl:`&GlobalResource`; this case is currently only
implemented for the ICON grid (use :nl:arg:`&GlobalResource icon_grid_description`
to load the description of the source grid).

Re-gridding on a regular grid is required before further processing of any field
defined on an unstructured grid.

Source points where the field is undefined never contribute to the interpolated
value; this means in particular that the 'next_neighbour' method may select 
a contributing grid point which is *not* the nearest neighbour (e.g. when the 
value of the field to interpolate at the nearest grid point is undefined)

A different re-gridding algorithm may be defined for each input / output pair.

.. note:: distances are to be understood as distance in (i,j) space, and not as geometric distances.

.. note:: bilinear interpolation has to be prefered to linear_fit when the target
          grid resolution is significantly finer than the source grid resolution. 
          Result of bilinear interpolation is also more robust than result of a 
          (localized) linear fit, which depends on the definition of the
          neighbourhood, and may produce values outside of the range of
          the source values.

.. note::
      the following special cases are automatically detected, and a more
      efficient algorithm is used:

      (crop)     
                  target mesh is a sub-mesh of input mesh, and the interpolation
                  method is "next_neighbour"
      (2d shift) 
                  target mesh is obtained by shifting the source mesh by an
                  amount smaller than the mesh size, either in i or in j
                  direction, and the interpolation method is a (weighted)
                  mean of the two nearest points (in particular one of
                  "average" or "dist_average" must be used)

.. note::
      when 'additional_diagnostic' is set to true, the following information
      is available in the file fieldextra.diagnostic (when re-gridding from 
      a regular grid):

      * location of target grid in source coordinate system
      * location of center of source grid in target coordinate system
      * neighbourhood size in terms of source grid element size
      * average number of source points contributing to each target value

      It is advised to check this information when defining a new re-gridding
      transformation!

.. note::
      it is possible to work on a subdomain of the grid specified by
      :nl:arg:`in_regrid_target`, by setting the values of imin, imax, jmin, jmax
      (see later on for the definition of these parameters)

.. note::
      when re-gridding from unstructured grid to regular grid with the
      "rbf" method, it is possible (but not recommended) to specify the
      scale factor. When not specified, the scale factor is internally
      computed, and the (range of) computed value can be printed out by
      setting ictools_debug_level=1 in :nl:nl:`&RunSpecification` (look for 
      "min/max shape parameter:" in fieldextra standard diagnostic,
      the shape parameter being the inverse of the scale factor).

Ensembles
^^^^^^^^^

Fieldextra supports composed EPS, i.e. ensembles build by merging different
model runs. For example, it can be a lagged ensemble, built from a collection
of determinist runs with differing start times, or it can be the merging of
two or more standard EPS runs.

Each model run contributing to such a composed EPS is tagged by a unique
'group' value. 

.. nl:argument:: epsgroup

    :format: <group id; strictly positive integer>


.. nl:argument:: epsgroup_size

    :format: <nbr. of EPS members associated with epsgroup; integer>

.. nl:argument:: epscomposed_size

    :format: <total nbr. of EPS members in composed system; integer>


    When set, the product category of all fields contributing to the specified
    output is set to eps_member, and the member identity is built from the
    specified epsgroup value and from the existing member identity (if any).
    The value of :nl:arg:`epsgroup_size` and :nl:arg:`epscomposed_size` must always be set.


Global operations
^^^^^^^^^^^^^^^^^

Besides the iterative processing of each extracted fields, described in 
:ref:`section 4.3.4` and :ref:`section 4.3.5`, it is possible to define some global operations
applied just before producing the output data ...

**n to m operator, for transformations not well suited for an implementation**

based on a combination of standard operators.  These n to m operators can be
implemented within fxtr_operator_generic or fxtr_operator_specific, and are
triggered by setting the value of out_postproc_module.

.. nl:argument:: out_postproc_module

    :format: <name of procedure to apply>


**re-gridding operator, to re-grid all fields available in the last processing iteration.** 

.. nl:argument:: out_regrid_target

    :format: <target grid for re-gridding>

.. nl:argument:: out_regrid_method

    :format: <re-gridding algorithm>

with the following optional attributes:

.. nl:argument:: out_regrid_method_distance

    :format: <weight_ij=positive_integer,weight_z=positive_integer>

    generalized distance

.. nl:argument:: out_regrid_topography

    :format: <source:incore_tag_s,target=incore_tag_t>

        
.. nl:argument:: out_regrid_method_filter

    :format: <source:incore_tag_s,target=incore_tag_t>

    filter to select contributing source points

The same usage as for :nl:arg:`in_regrid_target` / :nl:arg:`in_regrid_method` is supported 
(including :nl:val:`out_regrid_method='default'`). In addition, a different 
distance than the the lateral distance in ij-space can be specifid
(:nl:arg:`out_regrid_method_distance`) and the set of source points contributing
to each target point can be further restricted (:nl:arg:`out_regrid_method_filter`).


When :nl:arg:`out_regrid_method_distance` is specified, the distance "distance_ij" in
(i,j) space used by the regridding operator is replaced by a generalized distance.
The generalized distance is defined by:

``weight_ij*distance_ij + weight_z*d_height``      

where ``d_height`` is the height difference between source and target topography

The value of :nl:arg:`out_regrid_topography` must be set when :nl:arg:`out_regrid_method_distance`
is used. Both incore_tag_s and incore_tag_t must refer to INCORE fields 
representing the source topography (incore_tag_s) or the target topography
(incore_tag_t).

Usage of generalized distance is restricted to the following methods:
    next_neighbour, dist_average, exp_weight .


When :nl:arg:`out_regrid_method_filter` is specified, the source points contributing 
to each target point are those with the same value of "incore_tag_s" as the 
target value of "incore_tag_t". Both incore_tag_s and incore_tag_t must refer
to INCORE fields taking integer values, defined everywhere, and associated with
the source grid (incore_tag_s) or the target grid (incore_tag_t).

Note that the set of source points contributing to each target point is always
restricted to the neighbourhood specified in :nl:arg:`out_regrid_method`, independently
on the use or not of :nl:arg:`out_regrid_method_filter`.

Usage of filter is restricted to the following methods:
    next_neighbour, average, dist_average, log_average, exp_weight, linear_fit,
    dominant .


It is possible to specifiy a domain subset in addition to the regridding
algorithm, but only if the subset is compatible with a rectangular domain.
In such a case, re-gridding is performed after the data reduction associated
with the domain subset.

The domain subset may be a rectangle with a non trivial stride (``iinc/=1`` or
``jinc/=1``); this can be used to implement a field upscaling (see the note
about ``tiled_*`` operators).

Data reduction to the smaller sudomain contributing to the target grid,
including the halo for applying :nl:arg:`out_regrid_method`, is automatically applied.
However, this automated data reduction is inactive when a domain subset is
explicitely specified, or when the amount of data reduction is not significant.


Meta-information
^^^^^^^^^^^^^^^^

Some meta-information can be reset for all fields contributing to
the curent output

.. nl:argument:: out_model_name

    :format: <registred model name; string>

    Reset value of model name and model type; value must be a registred model
    name (see the list in the description of :nl:arg:`&GlobalSettings default_model_name`).
    
    A side effect of re-setting the model name is to change the value of
    any other meta-information associated with the chosen model name; this
    concerns the originatingCenter, the originatingSubcenter, and the
    generatingProcessIdentifier. Furthermore, the product category of the
    processed field must be compatible with the new model name.
   
    When the originatingCenter is changed, make sure that all local meta-
    information codes are still valid; if necessary, use :nl:arg:`out_auxiliary_metainfo`
    to also adapt these local codes.
   
    The effect of :nl:arg:`out_model_name` has precedence over all resetting operators
    defined in :nl:nl:`&GlobalSettings`, but not over operators defined in the same 
    block (e.g. :nl:arg:`out_auxiliary_metainfo`) or over field specific resetting
    operators (e.g. set_auxiliary_metainfo).

.. nl:argument:: out_genproc_type

    :format: <registred generating process type; string>


    Reset value of generating process type; value must be a registred
    generating process type (see the list in :nl:arg:`set_genproc_type`)
   
    The effect of :nl:arg:`out_genproc_type` has precedence over all resetting
    operators defined in :nl:nl:`&GlobalSettings`, but not over the field specific
    operator :nl:arg:`set_genproc_type`.
   
    See also the definition of :nl:arg:`set_genproc_type` for more information.

.. nl:argument:: out_product_category

    :format: <registred product category; string>


    Reset value of product category; value must be a registred product category
    (see the list in the description of "default_product_category")
   
    The effect of :nl:arg:`out_product_category` has precedence over all resetting
    operators defined in :nl:nl:`&GlobalSettings`, but not over the field specific
    operator "set_product_category".
   
    See also the definition of set_product_category for more information.

.. nl:argument:: out_vertical_coordinate

    :format: <tag of INCORE field>


    Reset vertical coordinates information, using the information associated with
    the specified INCORE field.

.. nl:argument:: out_auxiliary_metainfo

    :format: <sring>[,<string> ...] (each string expressed as key=value)


    Reset specified field meta-information; override any key=value specified
    in :nl:arg:`&GlobalSettings auxiliary_metainfo`.


Fields filtering
^^^^^^^^^^^^^^^^

The set of fields effectively used for the current output file
can be filtred

.. nl:argument:: out_filter_cst
    :default: blank

    :format: <filtering condition; string>


    Filter the data written in the output file; acts on fields declared
    as constant in the time dimension.
   
    Can be either:
    
    ``"always"``
      always dismiss constant fields
    ``"keep_in_first_output"``
      dismiss constant fields, except in the first output of a temporal serie 
      (i.e. when :nl:arg:`out_file` is a generic name using a time stamp in its definition).

.. nl:argument:: out_filter_time
    :default: blank

    :format: <filtering condition; string>


    Filter the data written in the output file in the time dimension. 
    Unlike out_tstart... this operator does not depend on the presence
    and on the values of time stamps in the definition of :nl:arg:`in_file` and
    of :nl:arg:`out_file`. It is typically used when input files contain  multiple
    validation dates and the mechanism provided by out_tstart... can
    not be used.
   
    The exact format is ``out_filter_time = '<mode>; <t1>,... [;<units>]'``

    ``<t1>,...`` is a comma separated list of positive integers representing dates or times.

    ``<mode>`` is one of the following:

    ``"keep_time_of_day"`` 
      only keep time levels with a validation date matching one of the time in 
      ``t1,...`` where ``t1,...`` are interpreted as time of day. Setting ``<units>`` is required in this case.
    ``"keep_validation_date"`` 
      ``t1`` are interpreted as absolute date YYYYMMDDHHmm, in terms of year, month, day, hour, minute. ``<units>`` are ignored in this case.

    ``<units>`` is either ``"hour"`` or ``"minute"``, indicating how ``t1,...`` should be interpreted.


Algorithm supports
^^^^^^^^^^^^^^^^^^

Some computation algorithms require the specification of associated
parameters or support different realisations. This is controlled by the
following parameters ...


**computation of EPS derived products**

.. nl:argument:: out_mode_mismemb_limit
    :default: 100[%]

    :format: <threshold for missing member related exception>

.. nl:argument:: out_mode_mismemb_abort
    :default: .T.

    :format: <action by missing member exception>

    When computing eps derived products, an exception is detected when the number
    of available members is smaller than any one of the following values

    * ``(out_mode_mismemb_limit/100) * ensemble_size``
    * ``2``

    The action taken when an exception is detected depends on the value of :nl:arg:`out_mode_mismemb_abort`
    : either abort production of corresponding product,
    when this variable is true, or silently set the resulting values to undefined.
    Note that the evaluation of the missing members exception is done independently
    for each validation date (in particular to support lagged EPS).

.. nl:argument:: out_mode_weighted_member
    :default: .F.

    :format: <use weighted EPS members>

    Use cluster population as member weight when set to true

.. nl:argument:: out_mode_normalized_stdev
    :default: .F.

    :format: <normalize standard deviation>

    When set to true, the EPS standard deviation is normalized by the value of the
    EPS mean (the value is set to undefined when the mean is too close to zero).
                      
**computation of synthetic satellite products**

.. nl:argument:: out_mode_rttov_cloud
    :default: 'none'

    :format: <type of clouds in rttov computation>

    One of {'none','simple','complex'}. Should only be set to 'none' when no
    cloudy products are computed. See description of synthetic satellite products
    in section 5.1 for more details.
   
**computation of air density** 

.. nl:argument:: out_mode_ignore_wloading
    :default: .T.

    :format: <ignore water loading>

    When set to false, the list of hydrometeors available in the active model
    must be explicitely specified by setting the variable "hydrometeor" in 
    &ModelSpecification
   
**computation of algorithm for lapse rate base height correction** (:nl:val:`voper=translate_simple`)

.. nl:argument:: out_mode_hcor_dhmin
    :default: 50.

    :format: <min. topo difference>

   
**computation of divergence trend term CAT_DVT**

.. nl:argument:: out_mode_dvt_scale
    :default: 1.

    :format: <scaling factor>

   
**computation of relative humidity**

.. nl:argument:: out_mode_rh_clipped
    :default: .T.

    :format: <relative humidity clipped at 100%>

   
**computation of heat index**

.. nl:argument:: out_mode_hi_adjusted
    :default: .F.

    apply NOAA adjustments to heat index

   
**computation of maximal sunshine duration**

.. nl:argument:: out_mode_sun_topo
    :default: .F.

    consider topo derived horizon

   
**computation of height of zero degree centigrade isotherm**

.. nl:argument:: out_mode_h0cl_extrapolate
    :default: .F.

    extrapolate below lowest model level

   
**computation of wet bulb temperature**

.. nl:argument:: out_mode_tw_iterative
    :default: .F.

    compute wet bulb temperature with a secant method

   
**neighbourhood probability algorithm**

.. nl:argument:: out_mode_nbhprob_shape
    :default: 0

    :format: <shape of neighbourhood>

    ===== ============================
    value shape
    ===== ============================
    0     cylindrical shape
    1     ellipsoidal shape
    2     maximum in time shape
    ===== ============================
        
.. nl:argument:: out_mode_nbhprob_rxy

    :format: <spatial radius; no default>

    in mesh size units

.. nl:argument:: out_mode_nbhprob_rt

    :format: <temporal radius; no default>

    in units of collected validation times

.. nl:argument:: out_mode_nbhprob_rfade

    :format: <radius of fading zone; no default>

    in mesh size units
   
**computation of Kalman filter and MOS correction**

.. nl:argument:: out_mode_kfcor_na
    :default: 'undef'

    handling of missing Kalman correction

    supported values are listed at :nl:arg:`out_mode_moscor_na`

.. nl:argument:: out_mode_moscor_na
    :default: 'undef'

    handling of missing MOS correction

    ======= =====================================
    value   description
    ======= =====================================
    'undef' set field value to undef
    'nocor' retain uncorrected field value [#]_
    'abort' stop processing of field
    ======= =====================================
   
    .. [#] 
        Since the product category is changed to 'stat_probability' when one of the 
        two operators 'moscor_logreg' and 'moscor_extlogreg' is applied, the original
        field value cannot be retained where MOS information is missing. The
        respective field values are set to undef instead.

The description of the concerned operators is done in :ref:`section 4.3.4`.


Out type
^^^^^^^^

The base format of the output file is defined by out_type; additional characteristics
and variants of the base format are controled by out_type_* variables, as explained
below.

.. nl:argument:: out_type

    :format: <output file type; e.g. GRIB2, ASCII_TABLE>

    Supported values of out_type:

    1.  Pseudo output type used as global internal storage

        ``"INCORE"``   : 
          used to:

          * associate grid points to specified locations
            (require fields HSURF and FR_LAND);
          * associate grid points to specified regions
            (require any field);
          * produce grid point height information for some output
            (require field HSURF);
          * specify model base grid when working with staggered fields
            (see :nl:arg:`in_regrid_target`);
          * specify model base grid when working with fields defined on a large domain
            (see :nl:arg:`in_regrid_target`);
          * specify grid and auxiliary fields for re-gridding
            (see :nl:arg:`in_regrid_target`, <out_regrid_target>);
          * merge different fields and define associated mask
            (see :nl:arg:`merge_with`);
          * compare different fields and define associated mask
            (see :nl:arg:`compare_with`);
          * provide access to additional constant fields 
            (see below).
     
          Automatic derivation of some constant fields is provided when the required parent
          is available in INCORE, as described in the table below. These fields are generated
          on the fly when requested (i.e. they are computed when collecting fields from INCORE
          storage).
          This mechanism is only active when all components of the fields to derive are missing
          in INCORE (e.g. HHL will not be derived if HFL is already present in INCORE).
      
          When a tag is associated with the parent field, the derived field inherits this tag.

          1.  These fields are always generated when parent is available; 
              they are generated in the listed order, and are immediately available for successive operations [#]_

              =============================================================== =======================
                derived field                                                 parent
              =============================================================== =======================
                HSURF (surface height [m])                                    FIS
                RLAT & RLON (geog. lat/lon [deg])                             HSURF 
              =============================================================== =======================
        
          2.  These fields are generated on-demand when parent is available

              =============================================================== =======================
                derived field                                                 parent
              =============================================================== =======================
                SWISS_WE & SWISS_SN (Swiss coord. [m])                        RLAT, RLON
                BOAGAW_WE & BOAGAW_SN (Gauss-Boaga coord., west sector [m])   RLAT, RLON
                BOAGAE_WE & BOAGAE_SN (Gauss-Boaga coord., east sector [m])   RLAT, RLON
                P0FL, HHL & HFL (pressure in [Pa], height in [m])             HSURF [#]_
                HEIGHT @ full model levels [m]                                HEIGHT @ half lev [#]_
              =============================================================== =======================

          .. [#]   
                this means, e.g., that HSURF, RLAT and RLON are generated when FIS
                is available in INCORE
          .. [#]
                only for COSMO model, with hybrid vertical coordinates, when
                vertical coordinate coefficients are available
          .. [#] only for COSMO model, when vertical coordinates are present

          .. note:: 
               it is possible to have multiple instances of these special fields in INCORE,
               but the fields associated with the reference grid must be marked by setting
               "reference_field" to false for all other instances.
          .. note:: the parameter :nl:arg:`out_file` may be omitted when out_type='INCORE'
          .. note:: mixing INCORE fields with other fields defined on a sub-mesh is supported (see the documentation of :nl:arg:`in_regrid_target`).
   
    2.  Pseudo output type used to extract the characteristics of the records contained
        in the associated input files
      
        ``INSPECT`` : 
                  the information produced is defined by the settings in :nl:nl:`&InspectSpecification`
                  and is stored in ASCII file fieldextra.inspect, in current working directory.
      
        When the only output in a namelist is the INSPECT pseudo-output, fewer consistency
        checks are performed in fieldextra and field transcoding is inactive. This supports
        a more transparent inspection of the input records. A contrario, this means that the
        INSPECT output produced in combination with other output reports interpreted values.
       
    3.  Spezialized ASCII output of common interest

        ``N_TUPLE``     : 
          set of events (fields values at a point in space-time), one event per line
        ``METEOG``      : 
          meteogram at a set of locations (old pseudo-ANETZ), one field per file
   
    4.  General purpose ASCII output

        ``ASCII_TABLE`` : 
          table, CSV, multiple options to arrange the values in the table
        ``TMPL_BASE``   : 
          form (defined by a user specified template, can be used to create XML files)
        ``BLK_TABLE``   : 
          set of records (2d horizontal slices), can also be imported by fieldextra
    
        The following are obsolete and will be removed in a future release

        ``FLD_TABLE``   : 
          table oriented, columns indexed by fields descriptor
        ``DAT_TABLE``   :                 
          columns indexed by validation date
        ``XLS_TABLE``   :                 
          columns indexed by location or location and field, support both formatted and comma separated values (csv)
   
    5.  General purpose binary output

        ``GRIB1``     : 
          WMO FM 92-IX Ext. GRIB (GRIB edition 1)
        ``GRIB2``     : 
          WMO FM 92-XIII Ext. GRIB (GRIB edition 2)
        ``NETCDF``    : 
          NetCDF, according to Climate and Forecast Metadata conventions (CF 1.7)
   
    6.  Output types that are only of local interest

        .. TODO: link
        
        See chapter 3. in README.user.locale. 
   

   
Out type attributes acting on file production
"""""""""""""""""""""""""""""""""""""""""""""
   

.. nl:argument:: out_type_replace
    :default: .TRUE.

    :format: <overwrite pre-existing output> 

    Compatible with ``GRIB1``, ``GRIB2``, ``BLK_TABLE``, ``N_TUPLE`` 

    By default, any pre-existing output file is over-written. It is possible
    to change this behaviour by setting out_type_replace=.false. to concatenate
    the new output at the end of the existing file; this is however only supported
    under the conditions: (1) out_type_snapshot is not used, (2) output support
    append mode.

.. nl:argument:: out_type_justontime
    :default: 'auto'

    :format: <'auto', 'no', or 'yes'>

    Compatible with ``GRIB1``, ``GRIB2``, ``BLK_TABLE``, ``NETCDF``

    Control the usage of just on time mode. In most cases it is enough to
    rely on the 'auto' mode; however, there are situations where the 'auto'
    mode incorrectly enable just on time mode. For example:

    1. all conditions for just on time are fulfilled (:ref:`section 4.3.2`), but
       some of the :nl:arg:`in_field` are not available for all processed time 
       planes, which will generate some sort of 'missing parent'
       exception when just on time is active.
       
    It could also be that the 'auto' mode is too conservative, and
    that the memory footprint can be drastically reduced by setting
    out_type_justontime to 'yes'. For example:

    2. input files belong to the same forecast serie, but are
       dispatched in distinct subdirectories.

.. nl:argument:: out_type_snapshot
    :default: -1

    :format: <time>,<time>,...

    -1 means no time list

    Compatible with any out_type

    It is possible to produce snapshots of currently processed output
    at specified times. This is for example useful when fieldextra is
    running concurrently to the model, and time critical output have
    to be generated before the end of the model integration.
    The specified list of times refers to the input time loop, as
    defined by tlist or tstart, ... A snapshot will be produced
    just after the processing of each corresponding input files. The
    name of each snapshot is build by appending out_snapshot_postfix and
    the value of the associated time stamp to the name of the output file.
    The value of :nl:arg:`&RunSpecification out_snapshot_postfix` must be defined in :nl:nl:`&RunSpecification`.
    The value of :nl:arg:`&RunSpecification out_noready_postfix` is ignored for snapshots.

.. nl:argument:: out_type_toc
    :default: .FALSE.

    :format: <generate table of content> 

    Compatible with ``GRIB1``, ``GRIB2``, ``NETCDF`` 

    A table of content of the output file may be generated on demand for some
    binary output (``GRIB1``, ``GRIB2``, ``NETCDF``). For each data object stored in the
    output file, a description of the object is written in the associated table
    of content (a data object is a GRIB record or a NETCDF array).
    The description is made from the index of the object in the output file,
    the field tag or short name, the dimension of the object, the field long
    name and units, and the product description. 
    The table of content is saved in a file whose name is built from the name
    of the output by appending the postfix '.toc'.

   
Out type attributes acting on data
""""""""""""""""""""""""""""""""""

.. nl:argument:: out_type_dupcst
    :default: .FALSE.

    :format: <duplicate constant fields>

    Compatible with ``BLK_TABLE``, ``XLS_TABLE``, ``GRIB1``, ``GRIB2``, ``NETCDF``

    Internally, constant fields are associated with each validation date, 
    but, by default, only the first occurence is produced in output (first 
    in terms of validation date). When out_type_dupcst is true, each
    constant field occurence is produced.

.. nl:argument:: out_type_noundef
    :default: .FALSE.

    :format: <skip data when all values are undefined>

    Compatible with ``BLK_TABLE``, ``XLS_TABLE``, ``GRIB1``, ``GRIB2``, ``NETCDF``

    By default, data is produced independently of the definition status of
    the field values. Setting out_type_noundef to true changes this
    behaviour:

    + for record oriented output (GRIB, BLK_TABLE ...):
      skip record when all values of the associated field are undefined;
    + for array oriented output (NetCDF):
      skip validation date when all values of all processed fields are
      undefined for this validation date;

.. nl:argument:: out_type_undefcode
    :default: -99999.

    :format: <missing value code>

    Compatible with ``FLD_TABLE``, ``DAT_TABLE``, ``BLK_TABLE``, ``XLS_TABLE``, ``ASCII_TABLE``, ``TMPL_BASE``, ``N_TUPLE``

    Missing value code used in ASCII output is defined by out_type_undefcode.

    .. note::
          the value of out_type_undefcode can be overriden on a field basis
          by setting set_undefcode (``BLK_TABLE`` only).

.. nl:argument:: out_type_scanmode
    :default: 'native'

    :format: <scanning mode>

    Compatible with ``BLK_TABLE``, ``GRIB1``, ``GRIB2``

    The mapping of a rectangular domain in some output may be explicitely
    specified by :nl:arg:`out_type_scanmode`:

    *  the data order follows the original scanning mode 'native'
    *  the data order follows a user specified scanning mode
        | '+i/+j/consecutive_i' , '+i/+j/consecutive_j', 
        | '+i/-j/consecutive_i' , '+i/-j/consecutive_j', 
        | '-i/+j/consecutive_i' , '-i/+j/consecutive_j', 
        | '-i/-j/consecutive_i' , '-i/-j/consecutive_j', 

       where i represents west-east axis, and j represents south-north axis
    
.. nl:argument:: out_type_packing
    :default: 'standard'

    :format: <packing algorithm>

    Compatible with ``GRIB1``, ``GRIB2``

    GRIB output support multiple algorithms to pack the data set.
    Currently available are (nbit is an integer in [2...32]): 

    **for GRIB 1 and 2**

    ``'standard'``     
      grid point simple packing, 16 bits per value (default)
    ``'simple,nbit'``  
      grid point simple packing, nbit bits per value
    
    **for GRIB 2 only**

    ``'jpeg,nbit'``    
      grid point JPEG2000, nbit bits per value
    ``'aec,nbit'``     
      adaptive entropy coding, nbit bits per value
      (lossless data compression, see GRIB 2 data
      representation template 5.42)
   
    .. note:: jpeg and aec compression are done on top of simple packing
    .. note::
          to give an idea of the performance of the different algorithms,
          for a single COSMO-1 output file (1283 fields of dim 1158x774),
          compared with ``'simple,16'``:

          | ``'aec,16'``   generates a file x2.5 smaller, in x1.5 the time 
          | ``'jpeg,16'``  generates a file x3 smaller, in x3 the time 
    .. note::
          the value of :nl:arg:`out_type_packing` can be overriden on a field basis
          by setting :nl:arg:`set_packing`

.. nl:argument:: out_type_ncformat
    :default: 'nc3_64boffset'

    :format: <NetCDF file format>

    Compatible with ``NETCDF``

    This defines the NetCDF file format, one of:

    ``nc3_classic``   : 
      classic netCDF-3 format
    ``nc3_64boffset`` : 
      64-bit offset netCDF-3 format
    ``nc4_classic``   : 
      netCDF-4/HDF5 format, restricted to classic model 
   
    .. note::
          the ``'nc3_classic'`` format is the most compatible, the ``'nc3_64boffset'``
          format supports files larger than 2 GB, and the ``'nc4_classic'``
          format supports both files larger than 2 GB and file compression.
    .. note::
          in order to get good performances when choosing ``'nc4_classic'``, it 
          is strongly recommended to set :nl:val:`out_type_ncunlimitedtime=.false.`

.. nl:argument:: out_type_ncunlimitedtime
    :default: true

    :format: <logical>

    Compatible with ``NETCDF``
    
    Very bad write performances are observed when producing 'nc4_classic'
    format if the time dimension is unlimited; furthermore, unlimited time
    dimension may be detrimental to NetCDF file compression. For these
    reasons, it is possible to set :nl:val:`out_type_ncunlimitedtime=.false.`
    to prohibit unlimited time dimension. 
   
    Note however, that, setting :nl:val:`out_type_ncunlimitedtime=.false.` may lead
    to much larger memory footprint at run time (in the case multiple time
    levels are stored in the same output).

.. nl:argument:: out_type_ncdeflate

    :format: <compression factor in [1,9]>

    default undefined

.. nl:argument:: out_type_ncshuffle
    :default: false

    :format: <logical>

    Compatible with ``NETCDF``, but requires :nl:val:`out_type_ncformat="nc4_classic"`

    When :nl:arg:`out_type_ncdeflate` is defined, the NetCDF will be compressed.
    Variable data is compressed, not metadata such as attributes.
    Deflation level 1 is intended to compress faster than level 9.
    Deflation level 9 is intended to compress smaller than level 1.
    ALl levels use losless data compression.
   
    .. note::
          shuffling (:nl:val:`out_type_ncshuffle=.true.`) often improves compression
          for little cost.
    .. note::
          not using an unlimited dimension for the time
          (:nl:val:`out_type_ncunlimitedtime=.false.`) may improve compression.
    .. note::
          very good compression factor may be obtained with the lowest deflation 
          level. For example, the size of a full laf output from COSMO-1 is 
          reduced by a factor of about 4 when setting :nl:val:`out_type_ncdeflate=1` and
          :nl:val:`out_type_ncshuffle=.true.` (from 5.084GB to 1.330GB, the same file in
          GRIB 2 has a size of 2.310GB), for a 3.5 increase in computation time.

.. nl:argument:: out_type_simplenc
    :default: .FALSE.

    :format: <produce simple NetCDF>

    Compatible with ``NETCDF``

    In some cases a simplified NetCDF output can be generated. This is
    in particular the case when:

    1. all fields are defined on a geographical latitude / longitude grid:
       the use of auxiliary coordinates variables is not needed, and the latitude
       and longitude variables are 1-dimensional, resulting in a smaller and
       simpler NetCDF output.
    2. all fields use the same vertical dimension: the name of the vertical
       dimension must not be indexed.
   
    Fieldextra will generate such a simplified NetCDF, when the value of
    out_type_simplenc is set to true. 

.. nl:argument:: out_type_nodegeneratedim
    :default: .FALSE.

    :format: <ignore degenerate dimensions>

    Compatible with ``NETCDF``

    A NetCDF data is stored in a n-dimensional array, with the set of n-dimensions
    being dependent on the type of field. Normally all relevant dimensions are
    considered, even when the dimension is degenerate; this behaviour is changed
    by setting :nl:arg:`out_type_nodegeneratedim` to true. 
    Example: a single level EPS field is stored in an array of dimensions
    (t,m,y,x), m being the perturbation number dimension; if a different output
    is used for each member and :nl:val:`out_type_nodegeneratedim=.TRUE.`, the dimensions
    (t,y,x) will be used instead.

   
Out type attributes acting on meta-data
"""""""""""""""""""""""""""""""""""""""
   

.. nl:argument:: out_type_nousetag
    :default: .FALSE.

    :format: <always use short name, ignore tag>

    Compatible with ``FLD_TABLE``, ``DAT_TABLE``, ``XLS_TABLE``, ``ASCII_TABLE``, ``BLK_TABLE``, ``N_TUPLE``, ``NETCDF``
.. nl:argument:: out_type_alternate_name
    :default: .FALSE.

    :format: <use alternate short name>

    Compatible with ``FLD_TABLE``, ``DAT_TABLE``, ``XLS_TABLE``, ``ASCII_TABLE``, ``BLK_TABLE``, ``N_TUPLE``, ``GRIB1``, ``GRIB2``, ``NETCDF``

    When the name of the field is coded in the output file, the value of the
    short name is used, or, if set, of the field tag. When out_type_nousetag
    is true, the value of the tag is always ignored. When :nl:arg:`out_type_alternate_name`
    is true, and an alternate short name is defined in the active dictionary, the
    alternate short name is used instead of the short name.
    An alternate short name is defined when the attribute ``keyUsedInProgram`` is
    set in the active dictionary; in this case, the alternate short name is the 
    associated key.
   
    Example: 
             in the active dictionary, the key 'SP' has the attribute 
             ``keyUsedInProgram=FF``; this means that the name 'FF' must be used
             in the namelist and all associated resources, and that the default
             value of the field name in any output will be 'FF'. However, by
             setting out_type_alternate_name to true, the default value of the
             field will be 'SP'. 

.. nl:argument:: out_type_autoncname
    :default: .FALSE.

    :format: <generate object name>

    Compatible with ``NETCDF``

    Automatically generate NetCDF object name when multiple instances of the
    same field name is detected. The object name is generated by appending
    the string ``_n`` to the field name, where n takes one of the value 1, 2 ...
    This output type is not compatible with just on time mode.

.. nl:argument:: out_type_alternate_code
    :default: .FALSE.

    :format: <use alternate pds code>

    Compatible with ``GRIB1``

    When set to true, alternate GRIB 1 code found in active dictionary will be
    used; alternate code is defined in the dictionary by setting the attributes
    ``alternateTable`` and ``alternateParameter``. When no alternate code is defined,
    the usual code is applied.
   
    A similar feature is offered by :nl:arg:`use_alternate_code`, but on a field basis.
    The alternate code will be used as soon as one of the two switches is set.
   
    This may be used, e.g., to code probabilities differently than determinist
    products, without having to introduce an additional dictionary.

.. nl:argument:: out_type_transcode
    :default: .TRUE.

    :format: <transcode GRIB 1 pds>

    Compatible with ``GRIB1``

    Internally, fieldextra requires the GRIB1 pds to be compliant with the
    WMO GRIB1 standard. Therefore, non-standard coding of the pds is translated 
    on input and restored on output. Setting ``out_type_transcode =.false.`` forces
    fieldextra to write the GRIB1 pds in the way it is internally represented,
    whenever it is possible (otherwise, the original pds values are restored).
   
    Used e.g. for the processing of COSMO soil moisture level information.

.. nl:argument:: out_type_timebnds
    :default: .FALSE.

    :format: <produce time bounds>

    Compatible with ``NETCDF``

    Produce time bounds information. In case this is set to true, the cell
    method for the time dimension will be added as attribute of each field,
    and the time bounds attribute will be added to the time dimension.
    
    This is only compatible with a data set having the following 
    characteristics: (1) for each field, the type of statistical
    processing is the same for each lead time, (2) for each lead 
    time, all non trivial time range intervals are the same.

.. nl:argument:: out_type_prefered_tunits
    :default: ''

    :format: <string1,string2>

    Compatible with ``GRIB1``, ``GRIB2``
    
    Specify the prefered time units used to code the time range information
    (first element) and the lead time (second element) in the output file. 

    The following values are supported:
         {1sec, 1min, 10min, 15min, 30min, 1h, 3h, 6h, 12h, 1day}
      
    Note that the specified units will only be considered if compatible with the
    representation of the associated time value (e.g. a lead time of 30' can
    not be coded in GRIB1 with a '1h' units).
   
    Default value is '' (i.e. no prefered time units specified)

.. nl:argument:: out_type_nogridincr
    :default: .TRUE.

    :format: <ignore grid increment>

    Compatible with ``GRIB1``

    Grid increment in GRIB 1 output is coded in millidegrees, and may be inexact
    for high resolution grid. Because this information is anyhow redundant and can
    be re-computed it is safer to set the corresponding value to 0 (default behaviour).
    Defining out_type_nogridincr=.F. forces fieldextra to write the grid increments
    values in the GRIB 1 gds.

.. nl:argument:: out_type_novcoord

    :format: <control coding of vcoord description, one of {'yes','no','auto'}>

    Compatible with ``GRIB1``, ``GRIB2``, ``NETCDF``
   
    For sigma and hybrid coordinates, the description of the vertical coordinates is
    coded as a set of coefficients (in each GRIB record, or in a NetCDF variable). When
    :nl:arg:`out_type_novcoord` is 'no', fieldextra will produce this information in each GRIB
    record, or in a special 'vcoord' NetCDF variable; when :nl:arg:`out_type_novcoord` is 'yes',
    this information is skipped (GRIB 1, NETCDF) or restricted to records associated
    with a sigma or hybrid surface (GRIB 2).
   
    In addition, in some special cases, the vertical coordinates coefficients are coded
    in the value of the GRIB 2 second surface (for fields with passive second surface).
    This feature is turned off when ``out_type_novcoord = "yes"``.
   
    The default value 'auto', when not changed by setting default_out_type_n_novcoord
    or default_out_type_g_novcoord in &GlobalSettings, is 'no' for GRIB1 and GRIB2 output
    (i.e. vcoord are produced), and 'yes' for NETCDF output (i.e. no vcoord are produced).
   
    For NETCDF output, support for coding vertical coordinate coefficients is only
    available for the COSMO model (release 3.18 or better), independently from the
    value of :nl:arg:`out_type_novcoord`.

.. nl:argument:: out_type_stdlongitude
    :default: .FALSE.

    :format: <use standard coding of longitude>

    Compatible with ``FLD_TABLE``, ``DAT_TABLE``, ``XLS_TABLE``, ``ASCII_TABLE``, ``BLK_TABLE``, ``TMPL_BASE``, ``GRIB1``, ``GRIB2``, ``NETCDF``

    By default, coding of longitude meta-information in output is in [-180.,180.[;
    by setting :nl:arg:`out_type_stdlongitude` to true, longitude will be coded in [0.,360.[.
    Note that this switch does not affect the coding of any longitude field (such as
    RLON).

.. nl:argument:: out_type_mapcst
    :default: .TRUE.

    :format: <reset constant fields meta-info>

    Compatible with any :nl:arg:`out_type`

    When :nl:arg:`out_type_mapcst` is true, and when mixing constant and non constant
    fields in the same output, the following meta-information associated with
    the constant fields are set to the common value of the non constant fields:
    the lead time, the type of generating process.
    Constant fields meta-information is not changed when no common value is
    available. The operation is done at the very beginning of the processing
    iterations.

   
Out type attributes acting on data format
"""""""""""""""""""""""""""""""""""""""""
   
.. nl:argument:: out_type_fmt

    :format: <format of values in output file>

    :nl:arg:`out_type_fmt` is either a Fortran edit descriptor (``FLD_TABLE``, ``DAT_TABLE``,
    ``BLK_TABLE``, ``XLS_TABLE``, ``ASCII_TABLE``, ``TMPL_BASE``, ``N_TUPLE``) or a key defining
    a specific table format (``METEOG``, see table below). 
   
    Default values are :nl:arg:`out_type` dependent:
      | ``FLD_TABLE``, ``DAT_TABLE``, ``BLK_TABLE`` (default 6G13.5)
      | ``TMPL_BASE``, ``XLS_TABLE``, ``ASCII_TABLE``, ``N_TUPLE`` (default G13.5)
      | ``METEOG`` (no default, :nl:arg:`out_type_fmt` mut be set)
   
    **Table format keys for METEOG**

    A key defines the format used to print out data (data_format), the
    type of symbols used for special values (data_spec) and the line
    header (line_head). Possible key values are:

    ==================== ============ ============== =========== ==========
    nbr. fields in table  allowed key  data_format    data_spec   line_head
    ==================== ============ ============== =========== ==========
                1        f73_dh       F7.3           std         delta_h
                1        f73_h        F7.3           std         h
                1        f72_dh       F7.2           std         delta_h
                1        f72_h        F7.2           std         h
                1        f71_dh       F7.1           std         delta_h
                1        f71_h        F7.1           std         h
                1        f71_dh_prec  F7.1           rain        delta_h
                1        f71_h_prec   F7.1           rain        h
                1        f61_dh_flag  F6.1           flag        delta_h
                1        f61_h_flag   F6.1           flag        h
                1        i_dh         nint           std         delta_h
                1        i_h          nint           std         h
                1        i_dh_prec    nint           rain%       delta_h
                1        i_h          nint           rain%       h
                1        clc          clc            clc         delta_h
                2        ddff         ddff           std         delta_h
    ==================== ============ ============== =========== ==========
 
    where

    for data_format:
        | Fd.d   : floating point, according to Fd.d edit descriptor
        | nint   : nearest integer
        | clc    : for cloud cover data (NINT for data < 7, INT otherwise)
        | ddff   : for wind direction and speed (NINT, "/" to separate data)

    for data_spec:
        | std    : "NA" for undefined values
        | flag   : append "*" to value when flag is set, "NA" for undef. values
        | clc    : "." when value is 0, "NA" for undef. values
        | rain   : "." when value is smaller than 0.05, "NA" for undef. values
        | rain%  : "." when value is not available

    for line_head:
        | delta_h: location name / height diff. between loc. and orography
        | h: location height / location name

.. nl:argument:: out_type_template
    :default: ''

    :format: <path of template file>

    Compatible with ``TMPL_BASE``, ``GRIB2``
.. nl:argument:: out_type_nomatch_action
    :default: 'raise_exception'

    :format: <action by unresolved data container>

    Compatible with ``TMPL_BASE``

    The generation of some output is based on a pre-defined template file.
    The meaning of the template depends on the output type:

    ``TMPL_BASE`` : 
            the template is any ASCII file where values to be inserted by
            fieldextra are replaced by special keywords ('data container').
            The default action when finding unresolved data container is
            to raise an exception; it is however possible to control this
            behaviour by setting :nl:arg:`out_type_nomatch_action` to one of

            ``'raise_exception'`` 
              default
            ``'ignore'``          
              keep unresolved data container unchanged
            ``'set_undef'``       
              replace unresolved data container by some appropriate default value

            This allows an iterative construction of the template based file,
            which is necessary to generate multi-models products.
    ``GRIB2`` : 
            the template is a GRIB 2 file containing a generic record used
            by ecCodes to construct the final GRIB 2 record. When
            out_type_template is not set, the value of :nl:arg:`&GlobalResource grib2_sample` is used instead.

            Important: see remark in :ref:`section 7.12` about the choice of the GRIB 2 sample.

.. nl:argument:: out_type_file_header
    :default: '' 

    :format: <user specified file header>

    Compatible with ``BLK_TABLE``, ``FLD_TABLE``, ``DAT_TABLE``, ``XLS_TABLE``

    Replace the standard and automatically defined output type and output version 
    string, which is also the file header.

.. nl:argument:: out_type_text1
    :default: ''

    :format: <user defined text for some output>

.. nl:argument:: out_type_text2
    :default: ''

    :format: <user defined text for some output>

    Compatible with ``FLD_TABLE``, ``DAT_TABLE``, ``BLK_TABLE``, ``XLS_TABLE``, ``ASCII_TABLE``, ``TMPL_BASE``, ``METEOG``, ``NETCDF``

    Set user defined text in file header of some output:

    * ``METEOG`` : two first header lines
    * ``BLK_TABLE``, ``FLD_TABLE``, ``DAT_TABLE``, ``XLS_TABLE`` : 'Description' header line
    * ``TMPL_BASE`` : attributes 'user_text1' and 'user_text2'
    * ``NETCDF`` : global attributes 'title' and 'comment'

.. nl:argument:: out_type_depreciated
    :default: .FALSE. 

    :format: <use depreciated version of format>

    Compatible with ``BLK_TABLE``, ``XLS_TABLE``

.. nl:argument:: out_type_verbosity
    :default: 1 

    :format: <verbosity of output; one of 0,1,2,3,4>

    Compatible with ``BLK_TABLE``, ``FLD_TABLE``, ``DAT_TABLE``, ``XLS_TABLE``

.. nl:argument:: out_type_mminfo
    :default: 'auto'

    :format: <express date in minutes; 'yes', 'no' or 'auto'>

    Compatible with ``BLK_TABLE``, ``FLD_TABLE``, ``DAT_TABLE``, ``XLS_TABLE``

.. nl:argument:: out_type_pdfinfo
    :default: 'auto'

    :format: <display PDF related information; 'yes', 'no' or 'auto'>

    Compatible with ``BLK_TABLE``, ``FLD_TABLE``, ``DAT_TABLE``, ``XLS_TABLE``, ``ASCII_TABLE``

.. nl:argument:: out_type_epsminfo
    :default: 'auto'

    :format: <display EPS member information; 'yes', 'no' or 'auto'>

    Compatible with ``BLK_TABLE``, ``FLD_TABLE``, ``DAT_TABLE``, ``XLS_TABLE``, ``ASCII_TABLE``

.. nl:argument:: out_type_prodinfo
    :default: 'auto'

    :format: <display product information; 'yes', 'no' or 'auto'>

    Compatible with ``BLK_TABLE``, ``XLS_TABLE``, ``N_TUPLE``

.. nl:argument:: out_type_locinfo
    :default: .FALSE.

    :format: <additional information on locations>

    Compatible with ``XLS_TABLE``, ``ASCII_TABLE``

    when out_type_locinfo is true and locations are used to specify a domain subset, 
    the latitude/longitude/height of each location is written in the output file
    header (in addition to the corresponding values for the associated grid points).

Some of these parameters support an automatic mode (``'auto'``); in this case the
format is derived from the content of the internal storage, or from the definition
of the time loop in the namelist (:nl:arg:`out_type_mminfo`).
      
.. nl:argument:: out_type_separator

    :format: <field separator>  

    Compatible with ``XLS_TABLE``, ``ASCII_TABLE``

.. nl:argument:: out_type_cheader
    :default: '' 

    :format: <column headers>

.. nl:argument:: out_type_fusedt
    :default: .FALSE.

    :format: <fuse date & time info>

    Compatible with ``XLS_TABLE``, ``ASCII_TABLE``

.. nl:argument:: out_type_line_label

    :format: <define way data is tabulated - string>

.. nl:argument:: out_type_legend

    :format: <define type of header - string>

    Compatible with ``ASCII_TABLE``

The above parameters define the characteristics of CSV oriented output.

1.  ``XLS_TABLE``

    ``XLS_TABLE`` output can either be expressed as a MS Excel compatible csv
    data, or as a column oriented fixed format data. The active option is
    defined by the value of :nl:arg:`out_type_separator`.
    
    :nl:arg:`out_type_separator` is a single character used to separate fields in
    output; date information and time information are also represented
    in a form compatible with MS Excel. When :nl:arg:`out_type_separator` is missing
    or blank, output is column oriented and fields are separated by one or
    more blanks.
    
    The csv variant supports additional customizing:
    :nl:arg:`out_type_cheader` is a string specifiying the way data is arranged in lines and columns:
         
    ``'gp'``    : 
      each column is uniquely specified by a location (grid point)
    ``'gp+fd'`` : 
      each column is uniquely specified by a location and a field
    ``''``      : 
      default, same as ``'gp'``

    :nl:arg:`out_type_fusedt` is a logical switch controlling the fusion of date and time information into a single string. 

2.  ``ASCII_TABLE``

    :nl:arg:`out_type_separator` is a single character used to separate fields in
    output; default value is ';', blank value is supported.
  
    Five options for the arrangement of output values are possible; the option
    is choosen by the namelist :nl:arg:`out_type_line_label`:

    ``location_time``      
                          one new line for each location and each validation
                          time (outer loop on location), different fields in
                          different columns
    ``time_location``     
                          one new line for each location and each validation
                          time (outer loop on time), different fields in
                          different columns
    ``time_location_level`` 
                          one new line for each location, each validation time
                          and each level (outer loop on time, inner loop on 
                          levels), different fields (incl. EPS / pdf infos)
                          in different columns; requires unique set of levels
    ``field_time``          
                          one new line for each field and each validation
                          time (outer loop on field), different locations in
                          different columns
    ``time_field``          
                          one new line for each field and each validation
                          time (outer loop on time), different locations in
                          different columns
    ``time``              
                          one new line for each validation time, one new column
                          for each location and each validation time (outer loop
                          on locations)

    Default value is ``"location_time"``.
  
    In addition, four options for the file header can be chosen, controled by
    namelist out_type_legend:

    ``none``     
      no header at all
    ``standard`` 
      standard header
    ``detailed`` 
      currently same as "standard"
    ``extra``    
      minimal header in data file, full header as extra file

    Default value is ``"none"`` (if :nl:arg:`out_type_line_label` is not set or
    :nl:val:`out_type_line_label=location_time`) or "standard" otherwise.
   

   
Out type attributes for special situations
""""""""""""""""""""""""""""""""""""""""""

Some output types support special 'dialects' to make them consistent
with downstream applications. 

.. nl:argument:: out_type_lagranto
    :default: .FALSE.

    :format: <LAGRANTO compatible NetCDF output>

    Compatible with ``NETCDF``

    LAGRANTO is a Lagrangian analysis tool (trajectories) developed at ETHZ
    (see http://iacweb.ethz.ch/staff/sprenger/lagranto/) which requires input
    files expressed in NetCDF. The standard NetCDF CF format produced by
    fieldextra is not compatible with LAGRANTO when working with GME of IFS
    model output, and the switch :nl:arg:`out_type_lagranto` must be set to true in this
    case. In particular, :nl:arg:`out_type_lagranto` triggers the coding of (Ak,Bk)
    vertical coordinates coefficients and change the naming convention of
    some NetCDF dimensions.

.. nl:argument:: out_type_dwhhack
    :default: .FALSE.

    :format: <DWH compatible FLD_TABLE output>

    Compatible with ``FLD_TABLE``

    DWH is the MeteoSwiss database, and some naming conventions used in the
    standard fieldextra ASCII output must be changed to make ``FLD_TABLE`` output
    compatible with this software. In particular, :nl:arg:`out_type_dwhhack` triggers
    the use of special field descriptor for EPS probabilities.


For most output types it is possible to restrict the data to a subset
of the whole domain ...

**by specifying a frame** 

.. nl:argument:: frame_ngp

    :format: <frame width, in grid points>

    The frame is the set of grid points whose euclidean distance to
    the border of the original domain is smaller or equal to 'frame_ngp'

**by specifying a rectangle in grid point coordinates**
(dense subset when iinc=1 and jinc=1, sparse subset otherwise)

.. nl:argument:: imin

    :format: <minimal value of i-component>     
    
    -1 means no subdomain

.. nl:argument:: imax

    :format: <maximal value of i-component> 

.. nl:argument:: jmin

    :format: <minimal value of j-component> 

.. nl:argument:: jmax

    :format: <maximal value of j-component> 

.. nl:argument:: iinc

    :format: <i-increment> 

.. nl:argument:: jinc

    :format: <j-increment> 

**by specifying a region in geographical coordinates**

.. nl:argument:: region_bbox

    :format: <region_set_tag,region_tag>

    The subdomain will be the bounding box of the region, expressed as 
    the smallest rectangle in grid point coordinates containing the
    specified region. The region is defined as a polygon in geographical
    lat/lon coordinates; the values of 'region_set_tag' and 'region_tag'
    refer to the definition of regions in the :nl:arg:`&GlobalResource region_list` file (see :ref:`section 6.4`).

    ..
      TODO: references in above paragraph

**by specifying a set of grid point coordinates**

.. nl:argument:: ilist

    :format: <i-component>,<i-component>,...

.. nl:argument:: jlist

    :format: <j-component>,<j-component>,...

    Grid point coordinates either refer to the grid specified by
    :nl:arg:`in_regrid_target` or to the native grid of the input field when 
    this former variable is not set.

**by specifying a set of locations in geographical coordinates**

.. nl:argument:: locgroup

    :format: <location group>

.. nl:argument:: loclist

  Can be either

  *  ``<location id>,<location id>,...``
  *  ``<location name>,<location name>,...``

The set of locations is defined by a list of geographical locations,
which are either defined in the :nl:arg:`&GlobalResource location_list` file or automatically
generated as region representative (see description of :nl:arg:`&GlobalResource region_list`).
The list of locations can be specified by using the location id's,
or by exploiting the location aliases defined in the :nl:arg:`&GlobalResource location_list`
file (of the form ``<group>:<name>``). In this latter case, the location
group has to be specified with ``locgroup=...``, and the associated list
of names with ``loclist=...`` Only one group may be specified. If :nl:arg:`loclist`
is empty, all group members are selected.

The definition of a dynamic set of locations is supported by specifying
:nl:arg:`locgroup` and letting :nl:arg:`loclist` empty; in this case, and in this case only,
the set of locations considered for the current output are all the
locations belonging to the specified group which are also resolved. In
other words, all locations associated with empty regions, outside of 
incore grid, or outside of target grid are filtred out.

Geographical locations are associated to grid points according to the
algorithm defined by location_to_gridpoint in :nl:nl:`&GlobalSettings`. The
reference grid is always the common base grid associated with the output 
file (i.e. the grid specified with :nl:arg:`in_regrid_target`, or the grid common
to *all* collected fields). Consequently, when mixing staggered with
un-staggered fields, the coordinates of the grid point associated with
a specific location are the same on the un-staggered as on the staggered
grid (i.e. they are not re-adjusted to take into account the staggering).

**by specifying a slice, i.e. a path on the surface of the earth**

.. nl:argument:: slice

    :format: <slice_tag>

    A slice is a broken line on the surface of the earth. The value of
    slice_tag refers to the definition of the slice found in the file 
    specified by slice_list (as defined in :nl:nl:`&GlobalResource`). See :ref:`section 6.5` for more detailed information.
   
   
.. note::
      a subset specification is typically not compatible with all possible
      output types. In particular, the slice specification is only compatible
      with NetCDF output. The table in section 6 specifies which subset
      specification is compatible with which output type.

.. note::
      when out_type requires a (dense) rectangular subset, which is the
      case for {GRIB1,GRIB2,NETCDF,BLK_TABLE}, and the specified subset does
      not match this condition, the smallest (dense) rectangle containing
      the specified subset is created, at the end of the processing iterations,
      with undefined values at any points not part of the original subset.

.. note::
      when out_regrid_* is also active, the re-gridding algorithm is applied
      _after_ the data is reduced to the specified subset. Note that out_regrid
      is only possible with a subset compatible with a sparse or dense rectangle.

.. note::
      the order of the values in the output file depends on the type of subset
      and the type of output:

      .. flat-table:: 
        :header-rows: 1

        * - :nl:arg:`out_type`
          - subset
          - order
        
        * - ``GRIB1``, ``GRIB2``, ``BLK_TABLE``
          - all
          - follow native scanning mode or scanning mode specified by :nl:arg:`out_type_scanmode`

        * - ``NETCDF``
          - all
          - values stored as 2D/3D/4D array (slice: slice dimension, others: lat/lon dimensions)
        
        * - :rspan:`3` other
          - no subset, frame, rectangle
          - follow native scanning mode

        * - grid points, locations
          - follow order specified in namelist

        * - slice
          - follow slice


It is also possible to restrict the data to a subset of the whole domain
defined by some logical conditions on the extracted or generated fields:

.. nl:argument:: in_filter

    :format: <logical condition>     
    
    e.g. ``in_filter='FR_LAND>0.5 && T<273.0'``

.. nl:argument:: tmp1_filter

    :format: <logical condition>     

.. nl:argument:: tmp2_filter

    :format: <logical condition>     

.. nl:argument:: tmp3_filter

    :format: <logical condition>     

.. nl:argument:: tmp4_filter

    :format: <logical condition>     

.. nl:argument:: tmp5_filter

    :format: <logical condition>     

.. nl:argument:: out_filter

    :format: <logical condition>     


Each logical condition is applied at the end of the corresponding processing
iteration (see :ref:`section 4.3.4` and :ref:`section 4.3.5`), and all field values at the locations
where the condition is not satisfied are set to undefined. It is then possible
to filter out these points, and to keep only the locations where the condition
is true, by using the ``N_TUPLE`` output format (see :ref:`section 7.5`).

Logical conditions are expressed as a suite of n tokens, combined with the
logical operator ``&&`` or ``||``; the full syntax is detailed in :ref:`section 4.3.7`.
Each token must use a field tag (empty tag is not allowed in this case).
Conditions are evaluated independently for each validation time.

The condition associated with a specific token depends on the value of 
the field tag: 

1. when the field tag univoquely defines a field in the current iteration (e.g. ``FR_LAND``), the associated condition is applied to all fields;
2. otherwise (e.g. T) the condition is interpreted and applied separately 
   for each class of fields having the same characteristics (e.g. for each level),
   and is ignored for other fields.

.. note::
      because derived fields are only computed where all parents are defined,
      these filters also influence the subsequently generated fields

.. _section 4.3.3:

Extracted fields
----------------

For each pair of input/output file the set of fields to extract is defined
by specifying one or many ``&Process in_field=...`` blocks. The way this
information is interpreted is defined by the value of :nl:arg:`selection_mode`.

For each output file, all extracted fields from all contributing input files
are available for the generation of the final product.

The definition of the fields to extract for a specific output must be
orthogonal in the following sense: any available input field must match
at most one ``&Process in_field=... /`` specification. Consequently, it
is not possible to extract twice the same field even if different tags
or operators are subsequently applied to this field (this restriction
does not exist for the definition of generated fields, see :ref:`section 4.3.5`).

Fields declared as constant in time in the active dictionary (:nl:val:`constant=T`)
are processed in a special way when mixed with non-constant fields; the
validation date and the time range information of each constant field is
automatically reset according to the following rules:     

-   constant fields are valid for each validation date associated with 
    one of the non constant field, and only for these validation dates;
-   the lead time of each constant field is set to the common lead time
    of the non-constant fields, if possible, otherwise to 0;
-   the time period of each constant field is set to none.

Note that these rules do not apply for output composed of constant
fields only.


.. nl:argument:: selection_mode

    :format: <INCLUDE_ONLY, INCLUDE_ALL or EXCLUDE>


    This defines how the ``&Process in_field= ...`` specifications are
    interpreted:

    INCLUDE_ONLY :  
      only the specified fields are extracted and are
      available for further processing (default).
    INCLUDE_ALL  : 
      the specified fields are extracted and are available
      for further processing, in addition all other input
      fields are transfered in the output file.
    EXCLUDE      : 
      all the fields which do *not* match the specifications are extracted and are available for further processing.
   
    When using INCLUDE_ALL or EXCLUDE mode, it is recommended to explicitely
    specify :nl:arg:`in_size_field` to minimize the memory footprint (otherwise the
    maximum number of fields is assumed, as defined in fxtr_definition).
   
    Only INCLUDE_ONLY is supported when working with INCORE pseudo-input. 

.. nl:argument:: in_field

    :format: <short name of field to collect>        

.. nl:argument:: skip_time

    :format: <time>,<time>...

.. nl:argument:: keep_time

    :format: <time>,<time>...

.. nl:argument:: expect_missing_field
    :default: false

    :format: <not exception for missing field>


The identity of the field associated with the in_field is defined in the
active dictionary. Note that only short names are allowes as in_field
value (using a field tag will produce an exception).

__ALL\__ is a reserved name, used as wildcard, to extract all fields present
in the input file. It may be necessary to explicitely specify in_size_field
to minimize the memory footprint when __ALL\__ is used (otherwise the maximum
number of fields defined in fxtr_definition is assumed).

__ALL\__ can not be used when selection mode is set to EXCLUDE.

When the input files are defined with a time loop, it is possible to filter
the set of input files contributing to in_field by setting skip_time or
keep_time (only one must be used). The specified list of times refers to
the input time loop, as defined by tlist or tstart, tstop. When setting
skip_time, only the input files with a time stamp not in skip_time list
are considered; when using keep_time, only the input files with a time
stamp included in keep_time list are considered.
This is useful to write compact namelist, e.g., when some of the input
fields are not available for all lead times, or when a different field
is used for lead time 0, like SOSB_RAD and ASOB_S.

By default, an exception is raised when a specified :nl:arg:`in_field` is not found
in the associated :nl:arg:`in_file`, or when the record is rejected. This behaviour
can be changed by setting :nl:arg:`expect_missing_field` to true; this is useful e.g.
when missing fields are expected and are replaced by interpolated values
from contiguous time levels (see ``'fill_undef'`` in :nl:arg:`toper`).

.. nl:argument:: ignore_model_identity
    :default: .FALSE.
    :type: logical


    When true, the model name and the model type of the associated field are
    ignored, and the default model name and default dictionary, as specified
    in :nl:nl:`&GlobalSettings`, are used instead. Furthermore, the values of any other
    meta-information associated with the new model name are also reset (this
    concerns the originatingCenter, the originatingSubcenter, and the
    generatingProcessIdentifier).
   
    This operator is applied before any ``set_*`` operator.
   
    This operator supports mixing fields from different models, e.g. when
    interpolating one model output to the levels defined for a different
    model.

.. nl:argument:: ignore_vcoord
    :default: .FALSE.
    :type: logical


    When true, the vertical coordinate information of the associated field
    is ignored, and the information derived from the other contributing
    fields is used instead.
   
    This operator supports working with model fields defined on multiple
    sets of vertical coordinates, e.g. when interpolating one model output
    to a different set of vertical levels.
   
    Be careful not to mix in the same operation model level fields defined
    for different vertical coordinates (in such a case, having the same 
    surface value does not guarantee that the same level is considered).

.. nl:argument:: reference_field
    :default: is true

    :format: <true of false>


    Used to discriminate in INCORE storage between fields associated with the
    reference grid and other instances of the field (see :nl:arg:`out_type`).

.. nl:argument:: use_tag

    :format: <tag1, tag2, ... (string)>  
    :noindex:

    The set of fields collected from some input files can be filtred on the
    basis of their tags. This can be necessary when multiple instances of
    the same field are stored in the contributing input.

.. nl:argument:: product_category
    :default: 'all'

    :format: <product category>
    :noindex:


    The product category to select may be explicitely specified; 
    
    supported values are:
      | 'determinist',
      | 'eps_control', 'eps_member', 'eps_mean', 'eps_perturbation',
      | 'eps_spread',
      | 'eps_standard_deviation', 'eps_standard_deviation_normed',
      | 'eps_quantile', 'eps_quantile_difference', 
      | 'eps_probability', 'nbh_probability', 'stat_probability',
      | 'eps_probability_refdist',
      | 'eps_extreme_forecast_index', 'eps_shift_of_tail_index',
      | 'satellite', 'radar', 
      | 'all'            

.. nl:argument:: level_class
    :default: 'all'

    :format: <type of level to select>


    The type of levels to select may be explicitely specified

    supported values are:

    ============== ==============================================================
    Value          Description
    ============== ==============================================================
    'p'            atmospheric levels, pressure surface
    't'            atmospheric levels, temperature surface
    'theta'        atmospheric levels, potential temperature surface
    'pv'           atmospheric levels, potential vorticity surface
    'density'      atmospheric levels, density surface (air, tracer, chemical...)
    'z'            atmospheric levels, height surface
    'z_agl'        atmospheric levels, height above ground surface
    'z_agl_half'   atmospheric levels, height above ground surface, interface
    'z_amsl'       atmospheric levels, height above sea surface
    'z_amsl_half'  atmospheric levels, height above sea surface, interface
    'k'            atmospheric indexed model levels (hybrid, generalized height)
    'k_half'       atmospheric indexed model levels, interface
    'k_full'       atmospheric indexed model levels, layer
    'soil'         soil levels
    'slev'         atmospheric/surface single level fields
    'mlev'         atmospheric multi-level fields
    'all'          all
    ============== ==============================================================
   
    .. note:: 
      each supported level can be uniquely attributed to one of
      {'soil', 'slev', 'mlev'}        
      
      | and the following groups are disjuncts
      | {'soil', 'slev', 'k'}   
      | {'soil', 'slev', 'k_half', 'k_full'}

Levels
^^^^^^

**The levels to select can be defined by ...**

* a list of values

  .. nl:argument:: levlist

      :format: <level>,<level>,...   
      
      -1 means all available levels

* or by specifying start value, end value and increment

  .. nl:argument:: levmin

      :format: <min value>

  .. nl:argument:: levmax

      :format: <max value>

  .. nl:argument:: levinc

      :format: <increment>

.. note:: 
  for fields defined on layers, the values specified by levlist or
  levmin ... always refer to the top surface of the layer (e.g. the
  lowest layer of a field defined on hybrid model levels, with model
  level indices increasing with height, is selected by setting levlist=2)

The associated physical units may be specified by

.. nl:argument:: level_units 

  :format: <units>

  optional

  The possible values are:

  ==================================== ==================================
  Value                                Description
  ==================================== ==================================
  'none', 
  'Pa', 'hPa'                           pressure
  'm', 'cm', 'mm'                       height or depth
  'K', 'cK'                             temperature
  'mPVU'                                potential vorticity
  'kgm-3', 'gm-3', 'mgm-3', 'mugm-3'    density , mugm-3 = 10**-6 g/m**3       
  ==================================== ==================================
  
  When :nl:arg:`level_units` is missing, the following default units are assumed

  ================================= ==================================
  ================================= ==================================
  pressure                          [hPa]
  temperature                       [cK]   (i.e. 1E-2 [K])
  potential temperature             [cK]   (i.e. 1E-2 [K])
  potential vorticity               [mPVU] (i.e. 1E-9 [K m2 s-1 kg-1])
  density level                     [10**-6 gm-3] 
  height above ground / above sea   [m]
  sigma level                       [1E-4]
  hybrid level                      [1]
  depth below ground                [cm]
  depth below sea                   [m]
  satellite channel                 [1]
  ================================= ==================================
   
If the specified field is attached to a layer, and not to a level,
these values are interpreted as top of layers (in a geographical sense)
or as the values for the variable surface when the other surface is 
constant (e.g. a layer defined between a pressure surface and the ground).

**The EPS members to select can be defined by ...**

* a list of values

  .. nl:argument:: epslist

      :format: <member>,<member>,...  
      :noindex: 
      
      -1 means all available members

* specifying start value, end value and increment

  .. nl:argument:: epsstart

      :format: <start eps member>
      :noindex:

  .. nl:argument:: epsstop

      :format: <stop eps member>
      :noindex:

  .. nl:argument:: epsincr
      :default: 1

      :format: <increment eps member>
      :noindex:

PDF Characteristics
^^^^^^^^^^^^^^^^^^^

Characteristics of PDF related fields are specified by:

* for probability
        
  .. nl:argument:: prob_interval

      :format: <[x,y], [x,y[, ]x,y] or ]x,y[, as string>

      interval of values, expressed in field units

      with  x and y being either real numbers, or
      ``'-infinite'`` or ``'infinite'``

* for probability, depreciated
  interval lower and upper boundaries, expressed in field units,
  the boundaries are *not* included in the interval

  .. nl:argument:: prob_tlow
      :default: -infinite

      :format: <interval lower threshold (real)>

  .. nl:argument:: prob_thigh
      :default: infinite

      :format: <interval higher threshold (real)>

* for quantile
         
  .. nl:argument:: quantile

      :format: <quantile, real> 

      a value for a quantile, between 0 and 100

* for difference of quantiles
         
  .. nl:argument:: quantile

      :format: <quantile1,real>, <quantile2,real>
      :noindex:
      
      two quantile values, both between 0 and 100, representing
      the difference between quantile1 and quantile2

* for probability with respect to a reference distribution
         
  .. nl:argument:: ref_quantile

      :format: <order of reference quantile, integer> [,<number of reference quantile, integer>]

      the order of the quantile in the reference distribution and
      the total number of quantiles used to represent the reference
      distribution

* for shift of tail index
         
  .. nl:argument:: efi_order

      :format: <percentile fcst, integer>[,<percentile climate, integer>]

      two percentile values, both between 0 and 100; the first
      value is the forecast percentile and the second the climate
      percentile

Misc
^^^^

.. nl:argument:: tag

    :format: <tag associated with field (string, <= 24 characters)>


    Normally a single tag can be associated with each field. As long as
    not overwritten, tag is propagated from one iteration to the next
    iteration. Tags need not be unique, and the same tag may be used for
    multiple fields. Tag values starting with "__" are reserved.
   
    By default a single instance of each 2d field is allowed in the in_field 
    iteration. This can be changed by setting in_duplicate_field to true;
    in this case, tags may be inherited from the input file or may be
    programatically set:

    * when a standard file is both used as ouput and as input in the same run,
      tags defined in the last iteration of the output production are inherited.
    * when the input file is a NetCDF file, and the field short name differs from 
      the NetCDF variable name [#]_, the variable name is used to set the field tag.
    * when neither inherited nor user defined, tags may be programatically set
      by using out_autotag.

    .. [#] either a short name attribute is found or a varname_translation table is available
   
    Tags are *never* inherited from INCORE storage (but :nl:arg:`use_tag` can
    be set to filter the fields collected from INCORE storage).
   
    When computing a new field, tags are *not* transfered from the contributing
    parent fields. However, automatically derived fields in INCORE storage
    inherit the tag associated with the main parent (see INCORE description 
    in :ref:`section 4.3.2`).
   
    When keeping track of multi-level fields, only levels with identical tags
    are associated. The same is also true when keeping track of multiple
    members of an EPS data set.
   
    Tags are used to:

    * mark incore fields for later reference in the definition of standard 
      output generation;
    * mark fields for later reference in some operators of the present
      iteration;
    * mark fields for later reference in use_tag of the next iteration;
    * distinguish between multiple instances of the same field;
    * re-define field names in some output (BLK_TABLE, FLD_TABLE, DAT_TABLE,
      XLS_TABLE, ASCII_TABLE);
    * build TMPL_BASE output.

    .. note:: 
      for complex namelists, it is an advantage to systematically use tags;
      it is also recommended, in order to differentiate tags from short names,
      to use lower case for tags.

.. nl:argument:: shared_eps_member
    :default: false

    :format: <logical>


    Mark associated field as shared EPS member; this is used to share a single
    determinist or EPS member field with multiple members (e.g. to share common
    model level height).
   
    When a shared EPS member is part of a product with a unique EPS member
    identity, it inherits this identity.

.. _section 4.3.4:

Operations applied on extracted fields
--------------------------------------

The following only applies when :nl:val:`selection_mode=INCLUDE_ALL` or
:nl:val:`selection_mode=INCLUDE_ONLY`.

Each extracted field may be transformed by one or more operators, in the
order 

:nl:arg:`regrid`, ``set_*``, :nl:arg:`merge_with`, :nl:arg:`compare_with`, :nl:arg:`hoper`, :nl:arg:`scale`/:nl:arg:`offset`, :nl:arg:`voper`,
:nl:arg:`toper`, :nl:arg:`poper`, :nl:arg:`poper2`, :nl:arg:`poper3`, :nl:arg:`poper4`, :nl:arg:`poper5`, spatial filter, :nl:arg:`new_field_id`

(the order is hardcoded and does not depend on the way the namelist is defined). 
Each class of operators is applied in turn to all the relevant fields; 
within a class the processing order is arbitrary.

Some of these transformations can be iterated by using the :nl:arg:`tmp1_field`,
:nl:arg:`tmp2_field`, :nl:arg:`tmp3_field`, :nl:arg:`tmp4_field`, :nl:arg:`tmp5_field`, and :nl:arg:`out_field`
construct (see :ref:`section 4.3.5`).

.. nl:argument:: regrid
    :type: logical 

    re-gridding as defined by :nl:arg:`in_regrid_target` and :nl:arg:`in_regrid_method`

    Activate re-gridding of the associated input field onto the mesh specified by
    :nl:arg:`in_regrid_target`. When all input fields need to be re-regidded, it is possible
    to set :nl:arg:`in_regrid_all` to true instead.

Some field meta-information can be (re)set by the user; this will overwite
any existing value in the current record, as well as any default value set
in :nl:nl:`&GlobalSettings`. Depending on the information being set, this can be done
in the extraction phase (i.e. while collecting fields from input data) or
in the last processing iteration, before producing the output. 

Operations which are restricted to the extract iteration:

:nl:arg:`set_vcoord`, :nl:arg:`set_time_range`

Operations not restricted (can be defined both in the extract iteration and
in the last processing iteration - when defined in the extract iteration, the
operator is applied just after collecting the record, at the very beginning
of the processing, whereas, when defined in the last processing iteration, the
operator is applied at the very end of the iteration):

:nl:arg:`set_level_property`, :nl:arg:`set_epsmember_identity`,
:nl:arg:`set_units`, :nl:arg:`set_genproc_type`, :nl:arg:`set_product_category`, 
:nl:arg:`set_reference_date`, :nl:arg:`set_trange_type`, :nl:arg:`set_leadtime`
:nl:arg:`set_auxiliary_metainfo`

.. nl:argument:: set_units

    :format: <string>

    reset the field units, as declared in the active dictionary,
    with a user defined text (which should be compatible with
    UNIDATA Udunits package) ; this information is used at the 
    following places:

    * units in ``NETCDF`` output,
    * part of description string in table of content associated 
      with ``GRIB1``, ``GRIB2``, and ``NETCDF`` output.

.. nl:argument:: set_genproc_type

    :format: <string> 

    reset type of generating process; possible values are:

    'observation', 'climatology', 'initialization',
    'analysis', 'analysis_error',
    'forecast', 'bias_corrected_forecast', 'forecast_error', 
    'smoothed_forecast', 'smoothed_calibrated_forecast',
    'eps_forecast', 'eps_probability', 'neighbourhood_probability'
    (not available in GRIB 1; defined in code table 4.3 of GRIB 2, including reserve for local use; supported by dictionary)

.. nl:argument:: set_product_category

    :format: <string>

    reset product category; possible values are:

    | 'determinist',
    | 'eps_control', 'eps_member' [2]_, 'eps_perturbation' [2]_,
    | 'eps_mean', 'eps_spread', 'eps_standard_deviation',
    | 'eps_standard_deviation_normed',
    | 'eps_quantile' [2]_, 'eps_quantile_difference' [2]_,
    | 'eps_probability' [2]_, 'nbh_probability' [2]_ , 'stat_probability' [2]_,
    | 'eps_probability_refdist' [2]_,
    | 'eps_extreme_forecast_index',
    | 'eps_shift_of_tail_index' [2]_,
    | 'satellite', 'radar'

    .. [2]
      Associated meta-information (EPS member identity, probability
      threshold, quantile value, efi order) must be consistent after
      re-setting the product category to one of these values.

    The product category is used in particular to select the
    correct set of parent fields when computing a new field,
    and to choose the correct template for GRIB coding.

.. nl:argument:: set_epsmember_identity

    :format: <member value>[,<number of members>]

    reset value of EPS identity, either the value of the EPS member, or the value of the EPS member and the total number of EPS members

.. nl:argument:: set_vcoord
    :default: false

    :format: <logical>

    reset vertical coefficients, using values
    specified in active :nl:nl:`&ModelSpecification`;
    must be specified for all multi-levels in_field
    (see documentation of :nl:arg:`&ModelSpecification vertical_coordinate_coef` for more
    details)

.. nl:argument:: set_level_property

    :format: <string>

    reset level characteristics
    
    format is one of

    * ``<sfc_type1>[,<sfc_value1>[,<sfc_value2>]]``
    * ``<sfc_type1>[,<sfc_value1>],<sfc_type2>[,<sfc_value2>]``

    surface type (``<sfc_type1>``, ``<sfc_type2>``) is one of:
        | surface, mean_sea_level,
        | pbl_top, tropopause, max_wind_level,
        | cloud_base, cloud_top, atmosphere_top,
        | hybrid, generalized_height,
        | entire_atmosphere (sfc_type1 only),
        | temperature[K], temperature[cK] [#]_,           
        | theta[K], theta[cK],             
        | height_amsl[m], height_agl[m],
        | pressure[Pa], pressure[hPa], 
        | p_vorticity[mPVU],
        | density_base[mugm-3], density_top[mugm-3] [#]_
        | density_base[gm-3], density_top[gm-3]  
        | soil[m], soil[cm], soil[mm]

    .. [#] cK = 1/100 K
    .. [#] mugm-3 = 10**-6 g/m**3

    surface value range (``<sfc_value1>``, ``<sfc_value2>``) is defined by the following table
    
    ============================ =======================
    value                        range
    ============================ =======================
    surface                      none
    pbl_top                      none
    tropopause                   none
    cloud_base                   none
    cloud_top                    none
    atmosphere_top               none
    entire_atmosphere            none
    max_wind_level               none
    mean_sea_level               none
    hybrid                       in {1,mx_lev+1} [3]_
    generalized_height           in {1,mx_lev+1} [3]_
    height_amsl                  in {-1000,100000}[m]
    height_agl                   in {0,100000}[m]
    pressure                     in {0,1200}[hPa]
    temperature                  in {100,100000}[cK]
    theta                        in {100,100000}[cK]
    p_vorticity                  in {-10000,50000}[mPVU]
    density_base, density_top    in {0,10}[kgm-3]
    soil                         in {0,5000][cm]
    ============================ =======================

    .. [3] mx_lev is set in module fxtr_definition

    ..
      TODO: reference fxtr_definition (in whole file)

    .. note::
          when two surfaces are defined, the order of appearance
          does not matter; the order of the surface information in the 
          output will only depend on the type of output and on the 
          information available in the active dictionary.

.. nl:argument:: set_leadtime

    :format: <integer, in minutes>

    reset the lead time (expressed in minutes);

    * in the extract iteration (can be combined with set_reference_date,
      the latter being first applied): the lead time is reset, the 
      reference date is unchanged, the validation date is modified (sum
      of the lead time and the reference date), and the time range (if any)
      is adapted to remain the same with respect to the lead time;
    * in the last processing iteration (only one of set_reference_date
      and set_leadtime can be set): the validation date and the time
      interval (if any) are not changed, the lead time is reset to
      the specified value, and the reference date is adapted.

.. nl:argument:: set_time_range

    :format: <integer, in minutes>

    reset time range, value expressed as a string
    ``[x,y]`` with  ``x`` and ``y`` being real numbers, or ``x`` being ``'-infinite'`` 

    The first value defines the start of the interval, the second
    value the end of the interval, both with respect to the validation 
    date. When ``x`` is set to ``'-infinite'``, the start of the interval is
    the reference date. Values are expressed as minutes.
    Limited to extract iteration, only allowed when the type of
    time range is associated with an interval.

.. nl:argument:: set_trange_type

    :format: <string>

    reset the type of time range (type of statistical processing);

    possible values (see code table 4.10 of GRIB2 standard):
      'none', 'average', 'accumulation', 'maximum', 'minimum',
      'difference\_(end-start)', 'root_mean_square', 
      'standard_deviation', 'covariance', 
      'difference\_(start-end)', 'summation'

.. nl:argument:: set_reference_date

    :format: <integer, yyyymmddhhmm>

    reset the reference date (yyyymmddhhmm);

    * in the extract iteration (can be combined with set_leadtime,
      the latter being last applied): the reference date is reset, 
      the lead time is unchanged, the validation date is modified
      (sum of reference date and lead time), and the time interval
      (if any) is adapted to remain the same with respect to the 
      lead time;
    * in the last processing iteration (only one of set_reference_date
      and set_leadtime can be set): the validation date and the time
      interval (if any) are not changed, the reference date is reset
      to the specified value, and the lead time is adapted.

.. nl:argument:: set_auxiliary_metainfo

    :format: <sring>[,<string> ...] 
    
    each ``<string>`` expressed as ``key=value``

    reset specified auxiliary meta-information; see :nl:arg:`&GlobalSettings auxiliary_metainfo`
    for more details. When used in the last processing
    iteration, override any key=value specified in :nl:arg:`&GlobalSettings auxiliary_metainfo`
    and in :nl:arg:`out_auxiliary_metainfo`

.. note::
      additional operators which can be used to re-set some meta-information ...

      *   globally for all output defined in the active namelist, at the end of 
          the last processing iteration:

          :nl:arg:`&GlobalSettings originating_center`, :nl:arg:`&GlobalSettings production_status`, :nl:arg:`&GlobalSettings genproc_type`, :nl:arg:`&GlobalSettings auxiliary_metainfo`
      *   for all fields of a specified output, at the end of the last processing
          iteration:

          :nl:arg:`out_model_name`, :nl:arg:`out_genproc_type`, :nl:arg:`out_product_category`,
          :nl:arg:`out_vertical_coordinate`, :nl:arg:`out_auxiliary_metainfo`

.. note::
      these operators differ from :nl:arg:`default_product_category` and 
      :nl:arg:`&GlobalSettings default_model_name` and :nl:nl:`&GlobalSettings.default_product_category`
      
      the latter ones are used to supplement missing meta-information whereas the
      operators descibed here always re-set the corresponding meta-information.

.. nl:argument:: merge_with

    :format: <tag>

    define the layer field           

.. nl:argument:: merge_mask

    :format: <logical condition>

    defines where the layer field is used


    Conditionally replace the current field with the incore field defined by
    :nl:arg:`merge_with`. The operation
    takes place where the incore field is defined and the logical condition
    specified by :nl:arg:`merge_mask` is true.
   
    The mask is defined as described in :ref:`section 4.3.7`, 
    with the following restrictions:

    - tag must be present,
    - tag refers to incore fields,
    - all incore fields contributing to the condition must be
      defined on the same grid (in particular, staggering is *not*
      supported in this case).
   
    The layer field is either uniquely specified by its tag, the value of
    :nl:arg:`merge_with`, or by its tag and the characteristics of the current field
    (first using the short name and the level, and then also using the EPS
    and PDF identity if name and level are not enough to define a unique
    field). The current and the layer fields must have the same physical units.
    All involved fields must be defined on collocated grids, with a non empty
    intersection, but the domains may differ. Outside of the common domain, the
    current field remains unchanged.
   
    The attributes of the resulting field (pds,gds) are the attributes of
    the current field. 


.. nl:argument:: compare_with

    :format: <tag (or __INCORE__)>

    define the field(s) to compare with

.. nl:argument:: compare_fct

    :format: <comparison operator to apply>

.. nl:argument:: compare_mask

    :format: <logical condition>

    define where the comparison takes place


    Replace the current field with a function of the current field and of the
    incore field defined by :nl:arg:`compare_with`. The function to apply is defined by :nl:arg:`compare_fct`; when 
    :nl:arg:`compare_mask` is defined, the original field is only modified where the
    corresponding logical condition is true.
   
    The mask is defined as described in :ref:`section 4.3.7`, 
    with the following restrictions:

    - tag must be present,
    - tag refers to incore fields,
    - all incore fields contributing to the condition must be
      defined onm the same grid (in particular, staggering is *not*
      supported in this case).
   
    The incore field is either uniquely specified by its tag, the value of
    :nl:arg:`compare_with`, or by its tag and the characteristics of the current field
    (first using the short name and the level, and then also using the EPS
    and PDF identity if name and level are not enough to define a unique
    field). Both current and incore fields must have the same physical units.
    All involved fields must be defined on collocated grids, with a non empty
    intersection, but the domains may differ. Outside of the common domain, the
    resulting field is set to undefined.
   
    Instead of explicitely specifying the field to compare with, the expression
    :nl:val:`compare_with='__INCORE__'` may be used to automatically look for matching
    fields in incore storage; criteria for matching field is based on short
    name, level, EPS identity, and PDF identity as explained above.
   
    The attributes of the resulting field (pds,gds) will be the attributes of
    the current field.
   
    The values of the resulting field is computed according to :nl:arg:`compare_fct`:
    
    1.  ``"ratio; a1,a2,a3,b1,b2,b3"`` [4]_ [5]_
        ::

          result = (a1*field1 + a2*field2 + a3) / (b1*field1 + b2*field2 + b3)
        
    2.  ``"ratio_abs; a1,a2,a3,b1,b2,b3"`` [4]_ [5]_
        ::
        
          result = (a1*field1 + a2*field2 + a3) / (b1*|field1| + b2*|field2| + b3)

    3.  ``"min"`` [5]_
        ::

          result = MIN(field1, field2)

    4.  ``"max"`` [5]_
        ::

          result = MAX(field1, field2)
 
    ``field1``
      extracted field
    ``field2`` 
      INCORE field
    ``ai``, ``bi`` 
      real numbers

    .. [4]
      Result is 0 if one of the following conditions applies

      + ``numerator <= (mc_eps * denominator)``

      (in particular the result is 0 when a3==0 and the two fields are 0)

    .. [5]
      Result is undefined if one of the following conditions applies

      + ``denominator < (mc_eps * numerator)``, when relevant
      + at least one of ``field1`` or ``field2`` is undefined
      + mask is undefined or ``.false.``

      (in particular the result is undefined outside of the common domain of the considered fields)
    
    .. note:: the ratio/ratio_abs operators change the field identity, and :nl:arg:`new_field_id` must be set.

Horizontal Operators
^^^^^^^^^^^^^^^^^^^^

.. nl:argument:: hoper

    :format: <horizontal operator>

.. nl:argument:: hoper_mask

    :format: <logical condition>      
    
    e.g. ``hoper_mask='>0. && FR_LAND>0.5'`` 


The following horizontal operators are implemented


=========================== ========================================================================
operator                    description
=========================== ========================================================================
  'id'                      identity (default)

  'offset_to_reference,loc' compute the offset of a field, at each active grid point,
                            with respect to the value of the field at the specified
                            location 'loc'. 'loc' is either the location id or a location
                            alias of the form ``<group>:<name>``.
      
         'fill_undef,r,v'   replace undefined values with a value derived from
                            neighbourhood; when distinct from 1., the value of
                            v limits the action of the operator to locations
                            where enough defined values are present in the
                            neighbourhood (see below for more details).
      
          'destagger'       interpolate staggered field to mass points
                            (see :ref:`section 4.3.2` for a discussion of the usage
                            of this operator)
          'stagger,dir,v'   generate field on staggered grid, the staggering being 
                            specified by the value of dir ("i" or "j") and by the
                            shift v to apply (one of -0.5 or 0.5).
      
         's9'               9 points averaging, square mask
         'c5'               5 points averaging, cross mask
         'c13'              13 points averaging, cross mask
         'c41'              41 points averaging, cross mask
         'diskavg,r'        averaging on a disk of radius r (in mesh size unit)
         'diskmin,r'        minimum value on a disk of radius r (in mesh size unit)
         'diskmax,r'        maximum value on a disk of radius r (in mesh size unit)
         'diskcnt%,r'       frequency of active points in a disk of radius r (in mesh size unit)
         'sqavg,r'          averaging on a square of radius r (in mesh size unit)
         'sqmin,r'          minimum value on a square of radius r (in mesh size unit)
         'sqmax,r'          maximum value on a square of radius r (in mesh size unit)
         'sqcnt%,r'         frequency of active points in a square of radius r (in mesh size unit)
     
         'shifted_sqavg,R'  same as sqavg, but mask shifted in upper quadrant, 2*R must be integer
         'shifted_sqmin,R'  same as sqmin, but mask shifted in upper quadrant, 2*R must be integer
         'shifted_sqmax,R'  same as sqmax, but mask shifted in upper quadrant, 2*R must be integer
         'shifted_sqcnt%,R' same as sqcnt%, but mask shifted in upper quadrant, 2*R must be integer
     
         'tiled_sqavg,r'    same as sqavg, but only computed on a grid subset ('upscaling')
         'tiled_sqmin,r'    same as sqmin, but only computed on a grid subset ('upscaling')
         'tiled_sqmax,r'    same as sqmax, but only computed on a grid subset ('upscaling')
         'tiled_sqcnt%,r'   same as sqcnt%, but only computed on a grid subset ('upscaling')
     
         'regcnt,s'         count active points in regions of set s
         'regcnt%,s'        frequence of active points in regions of set s 
         'regsum,s'         region sum, value, using region set s
         'regavg,s'         region average, using region set s
         'regmin,s'         region minimum, using region set s
         'regmax,s'         region maximum, using region set s
         'regqtl,s,p'       region p-quantile, using region set s

    'regfit,s,func,ai,tag'  compute a fit of (f,tag) in each region of set s,
                            using the class of functions defined by func, and
                            returning the coefficient ai of the fit. Currenly
                            implemented are:
                              
                            * linear regression:  ``f = a1 + a2 * tag``
                              
                              ``func = 'linreg'`` , ai in {'a1', 'a2'}
                            
                            When the fit is not possible (less than 2 active
                            points, tag values are all identical), undefined
                            is returned.
                            Example: :nl:val:`hoper="regfit,reg_m27,linreg,a2,tp24h_mean"`
=========================== ========================================================================

**Index**

s    : 
  region tag, string
loc  :
      location tag
f    :
      the field being transformed
tag  :
      field tag refering to another field available in the current
      iteration (either single 2d field, or field matching the 
      properties of the currently processed field)                
r    :
      radius, integer compatible with format I8 
R    :
      radius, real 
d    :
      diameter, integer compatible with format I8 
v    :
      fractional value in [0.,1.]
p    :
      quantile, real in [0.,100.] compatible with format F14.0
dir  :
      coordinates direction, "i" or "j"
func :
      class of function     
ai   :
      function coefficient                                     


Undefined values are always ignored when computing a new value. In addition,
the set of points contributing to the new value can be controlled by setting
a logical condition with 'hoper_mask' (see :ref:`section 4.3.7` for the excat syntax);
in this case, only the points where the condition is true contribute to the new
value. Masking is supported by all operators except 'stagger', 'destagger' and
'offset_to_reference'.


The set of points where the original field is modified depends on the operator:

'fill_undef'    
  only where the original field is undefined;
'reg*'          
  at all points belonging to a region of set s;
'tiled\_*'       
  all points of the grid subset, where the original field is defined;
other          
  all points where the original field is defined.

.. note::
  Note about ``*cnt%`` operators:
  
  active points are grid points belonging to the chosen mask (square or
  disk of radius r, region) which are both defined and where hoper_mask,
  when present, is true; the frequency of active grid points is the ratio
  of the number of active grid points with the number of mask points 
  where the original field is defined (and not the mask size!).

.. note::
  Note about ``fill_undef`` operator:

  the new value is computed by averaging within a disk of radius r all 
  values which are both defined and where hoper_mask is true; a weighted
  average is computed, using an exponentially decreasing weight with a
  scale of 1 (in grid units), this guarantees that only the nearest
  defined neighbours influence the result. In addition, it is possible
  to limit the action of this operator to locations where the field
  is almost defined everywhere by setting a value of v < 1.0 (to avoid
  artifacts in regions where the field is noisy).

.. note::
  Note about ``stagger`` operator:

  * Parent field must be defined everywhere
  * Staggering is computed by averaging the two nearest neighbours, when
    available, or by cloning the boundary value

.. note::
  Note about ``destagger`` operator:
  
  The same effect can be applied by using :nl:arg:`in_regrid_method`

.. note::
  Note about ``tiled_*`` operators:

  * These operators implement a field upscaling.
  * These operators must be applied in relation with the definition of a
    sparse rectangular subset (imin, imax, iinc...). The increment of the 
    sparse subset, iinc and jinc, must be equal to twice the operator radius
    plus one (e.g. iinc=jinc=5 for "tiled_sqavg,2"). Both imin and jmin must
    be larger than the operator radius (e.g. imin, jmin > 2).  
  * No operation should be defined which postpone the lateral reduction of
    the field after the iteration where the tiled operator is used; this 
    concerns the usage of any :nl:arg:`hoper` and the usage of any named operator
    requiring a lateral halo (as defined in the active dictionary). The
    reason of these condition is that the undefined values created by the
    tiled operator makes the usage of any lateral halo meaningless and
    obscure.
  * The grid subset where the ``'tiled_*'`` operator is computed starts at
    (imin,jmin) and is composed of all points of the sparse rectangular
    subset (so that that each grid point contributes exactly once to the
    resulting field); all points outside of the grid subset are set to 
    undefined. 
  * The resulting field can then either be interpolated on a coarser grid,
    the grid where the field is defined, or each undefined value can be set
    to the nearest computed value. Both transformations can be implemented
    with the out_regrid_target / out_regrid_method operator, as explained
    here:

    1.  coarser grid: 

        + out_regrid_target defines the coarser grid, the resolution being
          ``(2*r+1)`` the original resolution, and the start point being the
          original start point shifted by r times the original resolution;
        + out_regrid_method is set to ``"next_neighbour,square,0.5"``.

    2.  original grid, with undef replaced by nearest value:

        + out_regrid_target is the same as the original grid;
        + out_regrid_method is set to ``"average,square,R"`` with 
          ``R = (r+0.1)`` .

.. note::
  Note about ``shifted_*`` operators:

  * The set of points contributing to the value of the new field at (i,j) is
    ::

      (i+di,j+dj)  di in {0... 2*R}, dj in {0... 2*R}

    One should note that the geographical direction of increasing i (or j)
    depends on the scanning mode used to code the fields records.
  * This class of operators can be used to upscale a field from a fine grid
    (resolution delta) to a coarse grid (resolution N*delta), by applying
    the operator with a radius of (N-1)/2, thinning the data set by setting ::

      imin=1, imax=..., iinc=N, jmin=1, jmax=..., jinc=N

    and interpolating the resulting fields on the coarser grid, using
    next_neighbour interpolation. 

.. note::
  Notes about ``regavg`` / ``regmin`` / ``regmax`` / ``regcnt``/ ``regcnt%`` / ``regqtl`` / ``regfit`` operators:

  * The file defining user defined regions is specified by :nl:arg:`&GlobalResource region_list`. 
    This file may contain multiple sets of regions, and the argument 's'
    defines which set is used in the computation.
  * All points of the same region have the same value after the transformation,
    namely the value computed by the region operator. Moreover, values at points
    associated with more than one region or not associated with any region are
    set to undefined.

.. note::
  Notes about ``regcnt`` / ``regcnt%`` / ``regfit`` / ``tiled_sqcnt%`` / ``sqcnt%`` / ``diskcnt%`` operators:
  * These operators change the field identity, and new_field_id must be set.

.. note::
  Note about ``regcnt`` / ``regcnt%`` operator:

  * the number of active points is the number of points within the region
    where the field is defined and the condition hoper_mask is true.
  * the value of ``regcnt%`` is computed by dividing the number of active points
    by the total number of points in the region.

.. note::
  Note about ``regqtl`` operator:

  * the p-quantile of a region is computed on the basis of the data set of 
    all points of the region (possibly after filtering with hoper_mask), and
    by supposing equiprobable values.
  * the quantile is computed in the same way as for EPS products, namely
    by linear interpolation between two anchor values.
  * the p-quantile is set to undef when the number of active points of the
    region is smaller than 2.

Linear Transformation
^^^^^^^^^^^^^^^^^^^^^

.. nl:argument:: scale

    :format: <field scaling> 

.. nl:argument:: offset

    :format: <field offset> 


    Linear transformation of the field according to ::
    
       field --> (field + offset) * scale

    ``offset`` and ``scale`` are real numbers

Vertical Operators
^^^^^^^^^^^^^^^^^^

.. nl:argument:: voper

    :format: <vertical operator>

.. nl:argument:: voper_lev

    :format: <level>,<level>,...
    
    target levels for :nl:arg:`voper`, integer array

.. nl:argument:: voper_lev_units

    :format: <units>              
    
    associated physical units, optional
    > 
    see :nl:arg:`level_units`

.. nl:argument:: voper_use_tag

    :format: <tag1, tag2, ...> 
    
    optional string; tag of auxiliary fields


The following vertical operators are implemented:

A.  Identity (default)

    ``'id'``         

B.  Compute the height or the pressure of a surface defined by a condition on some
    auxiliary fields (e.g. height of lowest 0C isotherm, height of maximum CAT in
    a specified layer)
    
    | ``'find_condition,cond,tag2,mode'``     or   ``'find_condition,cond,tag2,tag3,mode'``
    | ``'find_maximum,tag1,tag2,mode'``       or   ``'find_maximum,tag1,tag2,tag3,mode'``
    | ``'find_minimum,tag1,tag2,mode'``       or   ``'find_minimum,tag1,tag2,tag3,mode'`` 
    | ``'find_nonzero,tag1,tag2,mode'``       or   ``'find_nonzero,tag1,tag2,tag3,mode'``
    | ``'find_value,tag1=value,tag2,mode'``   or   ``'find_value,tag1=value,tag2,tag3,mode'``

    where the condition is specified by

    ``cond``   
          logical condition on a set of auxiliary fields
          (see :ref:`section 4.3.7`); all refered fields must be available
          in the current iteration, and, if associated with
          a multi-level field, defined on all levels where
          the parent field is defined
    ``tag1``   
          field on which the condition is tested; refers to
          any multi_levels field available in the current
          iteration; this field must be defined on all levels
          where the parent field is defined
    ``value``  
          real value, refers to the field tag1 
          where the vertical interval to consider when looking for the
          specified condition is defined by 
    ``tag2``   
          refers to any unique 2d field available in
          the current iteration, expressed in the same
          units as the parent field, and specifies the
          start of the interval
    ``tag3``   
          refers to any unique 2d field available in
          the current iteration, expressed in the same
          units as the parent field, and specifies the
          end of the interval. When missing, the interval
          extends form tag2 to the top of atmosphere
          (if :nl:val:`mode='up'`) or to the bottom of atmosphere
          (if :nl:val:`mode='down'`)
    ``mode``   
          is one of "up" or "down", and specifies how
          the interval is scanned when looking for the
          condition ("up" and "down" refers to geometric
          direction).

    This operator is applied either on the pressure field (P) or on the 
    height field (HEIGHT); the computed surface will be defined in
    pressure coordinates in the former case, in height coordinates in
    the latter case. The parent field and the auxiliary fields must be
    defined on model levels.
    
    The target surface is defined by finding the first level where the
    condition is satisfied, independently in each column, scanning the
    specified interval in the direction specified by the value of 'mode'.

    The condition is:

    (find_condition) 
      the condition 'cond' is true
    (find_maximum)   
      tag1 reaches its maximum in the interval 
    (find_minimum)   
      tag1 reaches its minimum in the interval
    (find_nonzero)   
      tag1 value differs from zero 
    (find_value)     
      tag1=value, where a linear interpolation of the pressure or the height 
      field with respect to tag1 is performed to find the exact location where the condition is fullfilled

    It may happen that the field associated with tag1 is degenerate in
    some section (same value on multiple adjacent levels). The target
    surface is never defined within such a section, but is defined at
    the transition between the degenerate and the non degenerate part
    of the profile.

    The level meta-information of the resulting field is set to 
    undefined; this means in particular that it is not possible
    to create a GRIB record for this field.

C.  Vertical interpolation of atmospheric fields from full to half levels,
    or from half to full levels (only for indexed model levels, i.e.
    either hybrid or generalized height based vertical coordinates)

    ``'intpl_k2kfull,mode'``   
                            from half to full levels

                            * mode=average: use arithmetic mean of adjacent
                              levels values 
                              (# target levels = # parent levels - 1)
    ``'intpl_k2khalf,mode'``   
                            from full to half levels

                            * mode=average: use arithmetic mean of adjacent
                              levels values 
                              (# target levels = # parent levels - 1)
                            * mode=deaverage_bottom_0: bottom level value 
                              (in a geometrical sense) is set to 0, other
                              levels values are choosen such that the mean
                              of the two levels i and i+1 is the value of
                              the parent level i.
                              (# target levels = # parent levels + 1)
                                
    .. note::
          this operator can be used in particular to compute the HEIGHT
          field on model half levels when the height field is only available
          on model full levels (HFL --> HHL), and vice versa (HHL --> HFL).

D.  Vertical interpolation of atmospheric fields from model levels to specified
    height levels.

    Target levels are either specified by :nl:arg:`voper_lev`...
      from model level to height above ground
        | ``'intpl_k2h,mode'``  
        | ``'intpl_k2h+,mode'``
        | ``'intpl_k2h++,mode;extpl_mode,...'``

      from model level to height above sea level
        | ``'intpl_k2z,mode'``
        | ``'intpl_k2z+,mode'``
        | ``'intpl_k2z++,mode;extpl_mode,...'``
    
    \... or through a 'tag' refering to a height field (which can be either a single-level or a multi-levels field)
      from model level to height above ground
        | ``'intpl_k2h,tag,mode'``
        | ``'intpl_k2h+,tag,mode'``
        | ``'intpl_k2h++,tag,mode;extpl_mode,...'``

      from model level to height above sea level
        | ``'intpl_k2z,tag,mode'``
        | ``'intpl_k2z+,tag,mode'``
        | ``'intpl_k2z++,tag,mode;extpl_mode,...'``

    Four interpolation modes are supported:

    + linear in height (mode: ``'lin_h'`` or ``'h'``);
    + linear in pressure (mode: ``'lin_p'`` or ``'p'``);
    + linear in log pressure (mode: ``'lin_lnp'`` or ``'lnp'``);
    + take value on nearest model surface, independently in each
      column (mode: ``'nearest'``).

    ``'intpl_k2h+'`` and ``'intpl_k2z+'`` extend the operator with the
    possibility to interpolate between the lowest model level and
    a corresponding near surface field, linearly in height 
    (association between multi levels field and near surface field
    is based on declarations in the active dictionary).

    ``'intpl_k2h++'`` and ``'intpl_k2z++'`` extend the operator with the
    possibility to extrapolate the field profile vertically; the
    supported algorithms are:

    a.  extrapolationn below a specified height, keeping
        a prescribed field value constant:

        | z > start_sfc: f(z) = interpolated value
        | z =< start_sfc: f(z) = start_val
        
        the syntax of the operator is: ::

          voper='intpl...; extpl_down,constant,start_sfc,start_val'
        
    b.  extrapolationn below a specified height, using a
        prescribed start value and a prescribed gradient

        | z > start_sfc:   f(z) = interpolated value
        | z =< start_sfc:  f(z) = start_val + grad * (z - start_sfc)

        the syntax of the operator is: ::

          voper='intpl...; extpl_down,linear,start_sfc,start_val,grad'

    c.  (pressure only) extrapolation below a specified height,
        using a prescribed start value and the hydrostatic equation,
        for a temperature profile with a constant lapse rate

        | for t(z) = t_start + grad * (z - start_sfc)
        | z > start_sfc:   f(z) = interpolated value
        | z =< start_sfc:
        |    grad /= 0: f(z) = p_start * {t(z)/t_start}**(-g/R*grad)
        |    grad = 0 : f(z) = p_start * EXP{(-g/R*t_start)*(z-start_sfc)}
    
        the syntax of the operator is: ::

          voper='intpl...; extpl_down,hydstat,start_sfc,p_start,t_start,grad'
        
        or, for a lapse rate which is 0 everywhere::
        
          voper='intpl...; extpl_down,hydstat,start_sfc,p_start,t_start'

        The following notation is used:

        g,R      : 
          physical constants
        z        : 
          target height, in meter above mean sea level
        start_sfc: 
          field tag refering to a single height surface, expressed in meter above mean sea level
        p_start, start_val: 
          field tag refering to the start value of the field to extrapolate 
        t_start  : 
          field tag refering to the temperature field on start_sfc
        grad     : 
          field tags refering to field vertical gradient
        
        All mentioned tags must refer to fields present in the current
        iteration.

        Height field must be available on all levels where the field
        to interpolate is defined (when a tag is used to define the
        target levels, the height field must be distinct from the field
        refered to by this tag).

        When mode='lin_p' or mode='lin_lnp', the pressure field must be
        available on all levels where the field to interpolate is defined.

        The auxiliary field HSURF is required when some of the target
        levels are expressed in meter above ground (when a tag is used
        to define the target level, HSURF must be distinct from the field
        refered to by this tag).
        Note that HSURF can be *any* topography, not necessarily the
        model topo (use voper_use_tag to specify which HSURF to use
        when multiple choices are present).

        When one of the operator ``'intpl_k2h+'`` or ``'intpl_k2z+'`` is used,
        the associated near surface field must be available (e.g. T_2M
        when interpolating T).

        When one of the operator ``'intpl_k2h++'`` or ``'intpl_k2z++'`` is used,
        all auxiliary fields used for the extrapolation mode must be
        available.
        
        When :nl:arg:`voper_lev_units` is missing, it is assumed that target
        level(s) are expressed in [m].

E.  Vertical interpolation of atmospheric fields from model levels to
    specified pressure levels.

    ``'intpl_k2p,mode'``         

    Three interpolation modes are supported:

    + linear in pressure (mode: ``'p'`` or ``'lin_p'``);
    + linear in log pressure (mode: ``'lnp'`` or ``'lin_lnp'``);
    + take value on nearest model surface, independently in each
      column (mode: ``'nearest'``).

    Pressure field must be available on all levels where the field
    to interpolate is defined.

    When :nl:arg:`voper_lev_units` is missing, it is assumed that target
    level(s) are expressed in [hPa].

F.  Vertical interpolation of atmospheric fields from model levels to
    specified theta (potential temperature) levels.

    ``'intpl_k2theta,mode'`` 

    Interpolation is linear in theta. In case of folding of the target
    coordinates (multiple levels for the same theta), the value of mode
    defines the behaviour of the program: 

    ``'undef_fold'``: 
      the result is undefined,
    ``'low_fold'``  : 
      interpolation on (geometrical) lowest theta surface,
    ``'high_fold'`` : 
      interpolation on (geometrical) highest theta surface

    Potential temperature field must be available on all levels where
    the field to interpolate is defined.

    When :nl:arg:`voper_lev_units` is missing, it is assumed that target
    level(s) are expressed in [cK].

G.  Vertical interpolation of atmospheric fields from model levels to
    specified PV (potential vorticity) levels.
    ``'intpl_k2pv,mode'``    

    Interpolation is linear in pv.  In case of folding of the target
    coordinates (multiple levels for the same pv), the value of mode
    defines the behaviour of the program: 

    ``'undef_fold'``: 
      the result is undefined,
    ``'low_fold'``  : 
      interpolation on (geometrical) lowest pv surface,
    ``'high_fold'`` : 
      interpolation on (geometrical) highest pv surface

    Potential vorticity field must be available on all levels where
    the field to interpolate is defined.

    When :nl:arg:`voper_lev_units` is missing, it is assumed that target
    level(s) are expressed in [mPVU].

H.  Integral of soil fields in the layers defined by :nl:arg:`voper_lev`; the field to
    integrate must be an integral quantity (e.g. soil water content):

    ``'integ_soil'`` 

    source layers must be contiguous, boundaries of source and target
    layers are expessed as depth below surface.

    When :nl:arg:`voper_lev_units` is missing, it is assumed that target
    level(s) are expressed in [cm].

I.  (Normalized) integral of atmospheric field

    | ``'integ_sfc2h'``, ``'norm_integ_sfc2h'``,
    | ``'integ_sfc2z'``, ``'norm_integ_sfc2z'``

    Integral from the surface to the upper boundary specified by :nl:arg:`voper_lev`.
    Values of :nl:arg:`voper_lev` are expressed in height above ground (integ_sfc2h)
    or in height above mean sea level (integ_sfc2z); when multiple values
    of :nl:arg:`voper_lev` are defined, a new field is computed for each value.

    ``'integ_h2h,lbound,ubound'``, ``'norm_integ_h2h,lbound,ubound'``
                (lbound in height agl, ubound in height agl)
    ``'integ_h2z,lbound,ubound'``, ``'norm_integ_h2z,lbound,ubound'``
                (lbound in height agl, ubound in height amsl)
    ``'integ_z2h,lbound,ubound'``, ``'norm_integ_z2h,lbound,ubound'``
                (lbound in height amsl, ubound in height agl)
    ``'integ_z2z,lbound,ubound'``, ``'norm_integ_z2z,lbound,ubound'``
                (lbound in height amsl, ubound in height amsl)

    Vertical integral between lower and upper boundary, as specified by
    the arguments of the operator (:nl:arg:`voper_lev` is not supported for these
    operators). The integral is normalized by the height difference
    between upper and lower bounds when ``'norm_integ_*'`` operator is used:
    result = (1/ubound-lbound) * integral[lbound -> ubound](field(z) * dz) 

    Each of lbound and ubound may be expressed as an integer, or as
    a tag refering to a 2d field available in the same computing
    iteration. In this latter case, the associated field values must 
    represent a height above mean sea level or above ground (depending
    on the operator).

    When :nl:arg:`voper_lev_units` is missing, it is assumed that target
    level(s) are expressed in [m].

    .. note::
      The field to integrate may be defined on model full levels or on
      model half levels. The HEIGHT field must be provided on half levels,
      and must be available on the same set of levels as the field to
      integrate, or on one more level when the field to integrate is 
      defined on full levels.

      The auxiliary field HSURF is required when some of the boundaries
      are expressed in height above ground level. Note however that
      HSURF can be _any_ topography, not necessarily the model topo
      (use voper_use_tag to specify which HSURF to use when multiple
      choices are present).

      The field to integrate and the height field must be available and
      defined on all levels between lbound and ubound; at points where
      this condition is not satisfied, the result is set to undefined.
      When some of the boundaries are defined through field tags, the
      result is undefined where the refered fields are undefined.

J.  Minimum or maximum of atmospheric field in a specified height interval

    | ``'max_sfc2h'`` 
    | ``'max_sfc2z'``
    | ``'min_sfc2h'`` 
    | ``'min_sfc2z'``

    Maximum or minimum of the field computed for the height interval 
    starting at the surface and ending at the height specified in
    :nl:arg:`voper_lev`. Values of :nl:arg:`voper_lev` are expressed in height above ground
    (``*_sfc2h``) or in height above mean sea level (``*_sfc2z``); when multiple
    values of :nl:arg:`voper_lev` are defined, a new field is computed for each value.

    | ``'max_h2h,lbound,ubound'``,     (lbound in meter agl, ubound in meter agl)
    | ``'max_h2z,lbound,ubound'``,     (lbound in meter agl, ubound in meter amsl)
    | ``'max_z2h,lbound,ubound'``      (lbound in meter amsl, ubound in meter agl)
    | ``'max_z2z,lbound,ubound'``      (lbound in meter amsl, ubound in meter amsl)
    | ``'min_h2h,lbound,ubound'``,     (lbound in meter agl, ubound in meter agl)
    | ``'min_h2z,lbound,ubound'``,     (lbound in meter agl, ubound in meter amsl)
    | ``'min_z2h,lbound,ubound'``      (lbound in meter amsl, ubound in meter agl)
    | ``'min_z2z,lbound,ubound'``      (lbound in meter amsl, ubound in meter amsl)

    Maximum or minimum of the field computed for the height interval specified
    by the arguments of the operator (:nl:arg:`voper_lev` is not supported for these
    operators). Each of lbound and ubound may be expressed as an integer,
    or as a tag refering to a 2d field available in the same computing
    iteration. In this latter case, the associated field values must 
    represent a height above mean sea level or above ground (depending
    on the operator).

    When :nl:arg:`voper_lev_units` is missing, it is assumed that target
    level(s) are expressed in [m].

    .. note::
      The maximum and the minimum are computed for all model levels
      included in the specified height interval, but also consider
      the value of the field at the boundaries of the height
      interval; these latter values are obtained by linear
      interpolation with respect to the height.

      The field to reduce may be defined on model full levels or on
      model half levels. The height field must be provided, and must
      be available on the same type of levels and on the same set of
      levels as the field to reduce.

      The auxiliary field HSURF is required when some of the boundaries
      are expressed in height above ground level. Note however that
      HSURF can be _any_ topography, not necessarily the model topo
      (use voper_use_tag to specify which HSURF to use when multiple
      choices are present).

      The field to reduce and the height field must be available and
      defined on all levels between lbound and ubound; at points where
      this condition is not satisfied, the result is set to undefined.
      When some of the boundaries are defined through field tags, the
      result is undefined where the refered fields are undefined.

K.  Vertical derivative of atmospheric fields with respect to the height

    ``'derivate_k'``

    The field to derive must be defined on model full levels or on
    model half levels. Finite differences are used to compute the
    vertical derivative, either centered (within the profile) or
    left / right difference at profile ends.
    The height field must be accessible in the same iteration, and
    must be defined on the same set of levels as the field to derive.

    This operator changes the field identity, and new_field_id
    must be set.

L.  Upscaled vertical gradient of atmospheric field in a user specified
    height interval (algorithm taken from INCA software)

    | ``'upscale_grad_min,lbound,ubound'``
    | ``'upscale_grad_max,lbound,ubound'``

    Compute upscaled vertical gradient of the active field between
    lbound and ubound, where lbound and ubound are tags refering to
    single level height fields available in the current iteration.
    The upscaled gradient is computed by dividing the set of levels
    in two equal subsets, computing the vertical gradient between
    each surface of the first subset and the corresponding surface 
    of the second subset, and taking the minimum (upscale_grad_min)
    or the maximum (upscale_grad_max) of these gradients.

    The height field must be available on all levels where the field
    to transform is available. All heights must be expressed in
    meter above mean sea level.

    The upscaled vertical gradient is set to undefined at all points
    where less than 4 levels are within the prescribed boundaries.

    This operator changes the field identity, and new_field_id
    must be set.

M.  Translation of vertical structure to a different topography

    ``'translate_simple,to_location'``
    ``'translate_simple,to_grid:tag'``

    Transform a near-surface field; the field will be corrected to
    cope for a different topography than the original one. The 
    correction uses a lapse rate derived from the model profile
    (see COSMO PT CORSO-A, task 2). Three cases are considered:

    | let Dh = TOPO_target - TOPO_model
    | let ke be the index of the lowest model level

    1.  ABS(Dh) < dhmin [m]

        No corrections applied.

    2.  Dh < -dhmin [m]

        Lapse rate based on model levels (ke-2) and (ke-1).

    3.  Dh > +dhmin [m]
  
        Lapse rate based on value of field at height TOPO_target
        and value of field on level (ke-1), where the value of
        field at TOPO_target is obtained by linear interpolation
        between neighbour levels.

    Default value of dhmin is 50 [m], but this can be changed 
    by setting out_mode_hcor_dhmin.

    The target topography is either the altitude of specified locations
    (translate_simple,to_location), or an additional HSURF field
    refered to by the specified tag (translate_simple,to_grid:tag).

    The field to transform must be associated with a multi-level
    field, which is declared in the active dictionary (e.g. T_2M
    and T). The required parents are: the native topography HSURF,
    the field to transform, the associated multi-levels field and
    the height field. The associated multi-levels field and the
    height field must be defined on the same set of levels.


Vertical operators only act on levels sharing the same tags. When necessary,
the set of auxiliary fields required by the operator can be restricted to the
list defined by voper_use_tag: this list must be exhaustive, and it refers to
field tags in the current iteration.

.. note::
      to optimize code performance, extract parent fields on the 
      smallest possible number of levels (using levlist or levmin/levmax)

.. note::
      when one or more of the parents are undefined for a specific validation
      date, the derived field will also be undefined for this validation date.

Time Operators
^^^^^^^^^^^^^^

.. nl:argument:: toper

    :format: <time operator>     

.. nl:argument:: toper_mask

    :format: <logical condition>      e.g. toper_mask='lead_time>0'

 
The following time operators are implemented

**Identity**

``'id'``          
  identity (default)

**Operators acting on current time level only**

``'mask_all'``    
  set all field values to undefined

**Operators acting on a time interval**

``'max,t1,t2,t3,u'``         
  maximum value of interval
``'min,t1,t2,t3,u'``         
  minimum value of interval
``'sum,t1,t2,t3,u'``         
  sum of values of interval
``'avg,t1,t2,t3,u'``         
  average of values of interval, computed as weighted
  mean when the field is defined for non trivial time
  interval, the weight of each contributing time level
  being the associated time range, as arithmetic mean
  otherwise
``'date_of_max,t1,t2,t3,u'`` 
  date at which maximum value in interval is assumed
``'date_of_min,t1,t2,t3,u'`` 
  date at which minimum value in interval is assumed

* interval is defined by start time (t1), end time (t2), and increment (t3)
* interval refers to contributing time levels, relative to current validation time
* times can be expressed in ``'hour'`` or ``'minute'`` (u)
* increment can be omitted, in which case fieldextra consider all time levels
  included in the interval for which the considered field is defined; when no
  such time levels exist the result is undefined.
* when increment is specified, the value of the operator is undefined as
  soon as one of the specified time level is missing, or as soon as the
  considered field is undefined for one of the specified time levels.
* the time range of the resulting field is the specified time interval, except
  for the following cases

  - min of contiguous min is a min with a time range extending from start of
    first contributing time level to end of last contrbuting time level
  - max of contiguous max is a max with a time range extending from start of
    first contributing time level to end of last contrbuting time level
  - sum of contiguous accumulation is an accumulation with a time range
    extending from start of first contributing time level to end of last
    contributing time level

* the result of the operators ``'date_of_max'`` and ``'date_of_min'`` is the (real-valued) time
  index at which the maximum and, respectively, minimum value of the parent field is 
  assumed, i.e. a parameter of type TIME_INDEX and therefore requires the auxiliary 
  parameter TIME_INDEX to be defined in the dictionary

**Operators acting on the boundary of a time interval**

``'start,t1,t2,t3,u'``   
  start value of interval
``'end,t1,t2,t3,u'``     
  end value of interval

* these operators are useful to assign field values to different
  validation dates (e.g. assign daily 6h-6h average to 00 UTC of
  the first day, prepare parent fields for some operator depending
  on accumulated value and on start of interval value).
* interval is defined by start time (t1), end time (t2), and increment (t3)
* interval refers to contributing time levels, relative to current validation time
* times can be expressed in ``'hour'`` or ``'minute'`` (u)
* increment can be omitted
* only the start (resp. the end) of the interval matter; when the start (resp.
  the end) of the interval is not available the value of the operator is undefined.
* the time range of the transformed field is the one of the contributing time
  level, in particular the transformed field inherits the _absolute_ start and 
  end date of the contributing time level

**Differences**

``'delta,t,u'``   
  difference 
``'tdelta,t,u'``  
  weighted difference ::
    
    (t1*field(t1) - t2*field(t2))/(t1-t2)

  where t1 and t2 are lead times

* time offset (t) is expressed relative to current validation time
* times can be expressed in ``'hour'`` or ``'minute'`` (u)
* time offset and unit can be omitted, in which case contiguous time
  levels are considered; this is useful when time levels are not regularly
  distributed (but more error prone - so use it only when necessary!)
* when a time level is missing, e.g. for the first validation date, the
  result is set to undefined
* the time range of the resulting field is derived from the validation
  dates of the two contributing fields
* the time range type of the resulting field is given by the operator,
  except for the following special cases

  - difference of accumulations or of accumulation and instantaneous
    is again an accumulation
  - weighted difference of averages or of average and instantaneous
    is again an average

**Specialized differences**

``'delta_g'``   difference between two consecutive validation times, but 
            only when associated reference dates are identical and
            validation time differs from the common reference date;
            otherwise keep the field untouched.
``'tdelta_g'``  weighted difference between two consecutive validation times,
            but only when associated reference dates are identical and
            validation time differs from the common reference date;
            otherwise keep the field untouched.

* these operators can be used in the following cases

  - cumulated or averaged values from an intermittent assimilation cycle:
    the de-aggregation will be correctly calculated, independently of
    the implementation.
  - cumulated or averaged values from a forecast cycle, not defined at start:
    the de-aggregation will be available from the first validation date
    after start.
  - cumulated or averaged values with possible missing time levels:
    no information lost will result from the de-aggregation.

* these operators extract as much information as possible when
  de-aggregating the concerned field; this means (1) that time
  levels with values undefined everywhere are ignored (i.e. kept
  untouched by the operator, and skipped when looking for consecutive
  validation time), and (2) that the values of the first _defined_
  time level are kept untouched, except if this is the first element
  of the processed time serie, in which case values are set to
  undefined.
  Beware that the resulting field may contain differing time range
  length for different validation dates. 
* when two consecutive validation times can not be found, e.g. for the
  first validation date, the result is set to undefined
* the time range of the resulting field is derived from the validation
  dates of the two contributing fields
* the time range type of the resulting field is given by the operator,
  except for the following special cases

  - difference of accumulations or of accumulation and instantaneous
    is again an accumulation
  - weighted difference of averages or of average and instantaneous
    is again an average

**Interpolation**

``'fill_undef,method,r,u'``      
  method in {``persistance``, ``linear_interpolation``}
``'fill_undef,method'``          
  method in {``distribute``}
  This operator *only* acts on time levels where *all*
  field values are undefined (incl. the case where the
  field has never been set), replacing the undefined
  values with values interpolated from the neighbourhood.
  The type of interpolation is defined by the value of
  ``'method'``, which is one of {``persistance``, ``linear_interpolation``, ``distribute``}.
  The neighbourhood r is expressed as a difference in
  validation dates, which is an integer representing
  either hour (``u=hour``) or minute (``u=minute``).

* this operator can be used in combination with the clone_missing_input
  functionality, to support missing input files in a long time serie.
* this operator can also be used in combination with expect_missing_field
  to support missing time levels in input files for some specified fields.
  Note, however, that time levels for which *all* extracted fields are
  missing will not be created by the fill_undef operator and will not be
  present in output.
* ``'persistance'`` : for each undefined time level, find the nearest time level
  which is defined, looking both in the past and in the future, and use
  this value if the difference in validation dates is smaller or equal to r
  (otherwise the value remains undefined).
* ``'linear_interpolation'`` : for each undefined time level, find the nearest time 
  levels which are defined, both in the past and in the future (so called control
  points), and replaces the undefined value by the linear combination of the 
  control points. When some control point is missing, or when the difference in
  validation dates between the target time level and the nearest control point
  is strictly larger than r, the target time level remains undefined.
* ``'distribute'`` : this method is used when interpolating aggregated values
  (average, accumulation). Defined values are uniformly distributed over
  undefined time levels, such that aggregating the interpolated field 
  reproduces the original values. It requires that, for each time level,
  the end of the associated time range coincides with the validation
  date. Unlike the other methods, characteristics and values of defined
  time levels are also changed.
  More precisely, for each undefined time level find the nearest defined
  time level in the future, and, if the validation date of the undefined
  level is within the time range of the defined time level, associate a
  section of this time range to the undefined time level, using the
  assumption that the field is uniformly distributed. When some condition
  is not fullfilled, the field remains undefined.
  Example: accumulated precipitation, with lead times +3 and +4 hour
  missing, and with lead time +5 hour available and representing an
  accumulation from +2 to +5 hour (this configuration could be obtained
  after applying :nl:val:`toper=delta_g`): the ``'distribute'`` method will associate 
  1/3 of the +5 hour value to the interval +2 to +3 hour, 1/3 to the
  interval +3 to +4 hour, will change the time range of the +5 hour 
  to the interval +4 to +5 hour, and will change the value of the
  +5 hour to 1/3 of the original value.

**Operators depending on a field of type TIME_INDEX**

``'value_at_date,tag'`` value at date defined by tagged field (tag) of type TIME_INDEX

* the temporal information is inherited from tag


The set of time levels where the time operator is applied can be restricted
to the time levels fulfilling the condition defined by ``'toper_mask'`` (the mask
does not modify the set of time levels _contributing_ to the operator).
The syntax of ``'toper_mask'`` is a subset of the syntax described in section 4.3.7:    

* tag must always be present
* tag refers to a time attribute (and not to a field) and must be one of
      {``lead_time``}

The units for the numerical values used in the logical expression are the 
same as the ones defined for the time operator (argument ``'u'``), or [minute]
when missing (!).


Keep in mind the following rules when using a time operator:

* When set, an operator ignores time levels where "toper_mask" is false. 
* Except for ``'fill_undef'``, all operators always ignore time levels for
  which the concerned field has not been set, or for which the concerned
  field is everywhere undefined.
* When some of the contributing time levels are missing - the set of 
  contributing time levels being specified by the operator argument -
  the result of the operator is undefined. Note that some operators
  support a dynamic specification of the contributing time levels
  (e.g. "max,t1,t2,u", without t3 increment), in which case the 
  previous does not apply.
* In addition to the previous rule, the result of the operator is undefined
  at all locations where some of the operands are undefined. This also
  applies to the ``'fill_undef'`` operator.
* When looking for the nearest defined time level, all time levels which
  have been set and where the field is defined at least at one location
  are considered
* The reference time of the resulting operation is the minimum of the
  reference times of the contributing time levels, or, if required to
  keep the lead and the start time positive, the latest possible previous
  date.
   

Point Operators
^^^^^^^^^^^^^^^

 
.. nl:argument:: poper

    :format: <point operator> 

.. nl:argument:: poper2

    :format: <point operator> 

.. nl:argument:: poper3

    :format: <point operator> 

.. nl:argument:: poper4

    :format: <point operator> 

.. nl:argument:: poper5

    :format: <point operator> 


Unlike other operators, up to three different point operators can be defined
for each field; this supports more compact and comprehensible namelists.

As already mentioned, :nl:arg:`poper` is first applied  to all the relevant fields, then
:nl:arg:`poper2` is applied  to all the relevant fields, ... and finally poper 5 is applied
to all the relevant fields. That means in particular that the results of the
transformation by :nl:arg:`poper`, for any field, is available to :nl:arg:`poper2`, ... :nl:arg:`poper5`.

The following point operators are implemented:

**Identity operator (default):**

``'id'``                     

**Operators to transform an existing field according to some mathematical expression**

``'max,tag1,tag2'``
  maximum of 2 fields, the processed field is replaced by ``tag1`` if ``tag1 >= tag2`` and ``tag2`` otherwise.
  If ``tag1`` or ``tag2`` are undefined, the result is undefined.

  ``tag1`` and ``tag2`` are field tags defined
  in the current iteration; the field being
  is refered to by the tag ``'__self__'`` and must 
  be present. All contributing fields must have
  the same units.

``'min,tag1,tag2'``
  minimum of 2 fields, the processed field is replaced by ``tag1`` if ``tag1 <= tag2`` and ``tag2`` otherwise.
  If ``tag1`` or ``tag2`` are undefined, the result is undefined.

  ``tag1`` and ``tag2`` are field tags defined
  in the current iteration; the field being
  is refered to by the tag ``'__self__'`` and must 
  be present. All contributing fields must have
  the same units.

``'polynomial,a0,a1,...'``
  polynomial function of a field, the processed field is 
  replaced by: ::

    result(i,j) = SUM(k=0,n) (ak * field(i,j)**k) , n <= 3

``'logarithm_e'``
  natural logarithm of field, the processed field is 
  replaced by: ::

    result(i,j) = log_e(field(i,j))

  The result is undefined if ``field(i,j) <= 0``

``'exponential'``
  exponential of field, the processed field is 
  replaced by: ::

    result(i,j) = exponential(field(i,j))         
                
  The result is undefined if ``field(i,j) <= max_value``
  (``max_value``: largest real to avoid overflow)

``'product,tag'``     
  product of two fields, the processed field is
  replaced by: ::

    result(i,j) = field(i,j) * tag(i,j)  

``'quotient,tag'``          
  quotient of two fields, the processed field is
  replaced by: ::

    result(i,j) = field(i,j) / tag(i,j)

  The result is undefined when a zero divide exception occurs

``'sum,tag1*fac1+...'``    

``'sum,tag1/fac1+...'``    
  generic sum, the processed field is replaced by: ::

    result = SUM(i=1,n) (tagi opi faci)   

  where 

  *   ``opi`` is the mathematical operator ``'*'`` or ``'/'``
  *   ``tagi`` are field tags defined in the         
      current iteration; the field being
      processed is refered to by the tag
      ``'__self__'`` and must be present.
      All contributing fields must have
      the same units.
      'tagi opi' may be omitted, in this case
      the corresponding term is associated
      with a constant field.
  *   ``faci`` are real values and may be
      omitted 

``'inverse,tag1*fac1+...'``

``'inverse,tag1/fac1+...'``
  generic inverse, the processed field is replaced by: ::

    result(i,j) = 1. / SUM(i=1,n) (tagi opi faci)   

  where 

  *   ``opi`` is the mathematical operator ``'*'`` or ``'/'``
  *   ``tagi`` are field tags defined in the         
      current iteration; the field being
      processed is refered to by the tag
      ``'__self__'`` and must be present.
      All contributing fields must have
      the same units.
      'tagi opi' may be omitted, in this case
      the corresponding term is associated
      with a constant field.
  *   ``faci`` are real values and may be
      omitted 
            
  The result is undefined if a zero divide exception occurs.

``'ratio,tag1*fac1+...,tagj*facj+...'``

``'ratio,tag1/fac1+...,tagj/facj+...'``
  generic rational fraction, the processed field is replaced by: ::

    result = SUM(i=1,n) (tagi opi faci) /
              SUM(j=1,m) (tagj opj facj)  
          
  where 
  
  *   ``opi`` and opj are one of the mathematical operator ``'*'`` or ``'/'``
  *   ``tagi`` and ``tagj`` are field tags defined
      in the current iteration; the field
      being processed is refered to by the
      tag ``'__self__'`` and must be present.
      All fields contributing to the numerator
      must have the same units; all fields
      contributing to the denominator must
      have the same units.
      ``tagi opi`` and ``tagj opj`` may be omitted,
      in this case the corresponding terms are
      associated with a constant field.
  *   ``faci`` and ``facj`` are real values and may
      be omitted

  The result is undefined if a zero divide exception occurs.

``'norm,fmin,fmax'``
  field norm, the processed field is replaced by: ::

    result(i,j) = (field(i,j)-fmin(i,j))/(fmax(i,j)-fmin(i,j))  

  where ``fmin`` and ``fmax`` are field tags defined in the current iteration

``'stretch,ffix,fmax,dfmax'``
  stretch field in the interval ``[ffix,fmax]``, keeping the 
  field unchanged where ``field = ffix`` and stretching the
  field with a value ``dfmax`` where ``field = fmax``:

  If ``f_fix(i,j) >= f_max(i,j)``
    if ``field >= f_fix`` ::

      result(i,j) = field

    if ``field >= f_max`` ::

      result(i,j) = field + df_max * (f_fix-field)/(f_fix-f_max)

    otherwise ::

      result(i,j) = field + df_max

  If ``f_fix(i,j) < f_max(i,j)``
    if ``field <= f_fix`` ::

      result(i,j) = field

    if ``field <= f_max`` ::

      result(i,j) = field + df_max * (f_fix-field)/(f_fix-f_max)

    otherwise ::

      result(i,j) = field + df_max

  The conditions  ``f_fix /= f_max``  and  ``ABS(df_max) < ABS(f_fix-f_max)``
  must be fullfilled everywhere.


**Operators to manipulate undefined values:**

``'mask_all'``                 
  set to undefined all values of the field
``'mask,cond'``                
  set to undefined all values of the field where the logical condition ``'cond'`` is true. 
``'replace_undef,value'``      
  replace all undefined with the specified value.

**Operators to change field values at specified locations:**

``'replace_all,value'``         
  set value everywhere
``'replace_cond,cond,[value|tag]...'``  
  replace values at each location where one logical condition
  ``'cond'`` is true; the new values are either specified by a
  constant real number, or by a tag refering to another field
  in the same iteration. Multiple pairs of condition / value
  may be specified, but the regions in grid point space where
  the conditions are true must not overlap (evaluated 
  independently for each validation date). The values to 
  set are either specified by a constant real number, or by
  a tag refering to another field in the same iteration.
``'replace_ij,i,j,value...'``   
  replace value(s) at specified coordinates. Multiple coordinates can be specified.
``'replace_loc,loc,value...'``  
  replace value(s) at grid points associated with specified
  location(s). Multiple locations can be specified; either
  the location id or an alias of the form ``<group>:<name>``
  may be given.

**Operators to create a new field using the current field as template,
changing the values at specified locations and setting undefined elsewhere:**

``'create_cond,cond,[value|tag]...'``  
  set the resulting field to some specified values at each 
  location where a logical condition ``'cond'`` is true; set
  to undefined elsewhere. Multiple pairs of condition / value
  may be specified, but the regions in grid point space where
  the conditions are true must not overlap (evaluated
  independently for each validation date). The values to 
  set are either specified by a constant real number, or by
  a tag refering to another field in the same iteration.
``'create_cond,cond,value1,value2'``
                            set the resulting field to ``'value1'`` at each location
                            where the logical condition ``'cond'`` is true, set to ``'value2'``
                            where the condition is false, and set to undefined where
                            the condition is not defined (i.e. at any location where 
                            some of the fields used within ``'cond'`` are undefined).
``'create_ij,i,j,value,...'``   
                            set value(s) at specified coordinates, set to undefined
                            otherwise; multiple coordinates can be specified.
``'create_loc,loc,value,...'``  
                            set value(s) at grid points associated with
                            specified location(s), set to undefined otherwise;
                            multiple locations can be specified; either the location 
                            id or an alias of the form ``<group>:<name>`` may be given.

**Operators to transform an existing field according to pre-defined rules:**

``'n2geog[,tag1,tag2]'``        
                            transform vector components from native to geographical
                            reference system, characterized by easterly and
                            northerly directions (only for fields declared as 2d
                            vector components in the active dictionary, requires
                            access to both components). When definition of components 
                            is ambiguous, tags must be set for both components and
                            passed through the operator arguments tag1 and tag2.
``'azimut_class,n,cond'``       
                            transform the input value in azimut class according to 
                            the following definition: 

                            | n classes, from 1 to n
                            | first sector north, inverse trigonometric order
                              
                            For example, with n=8: 
                              1 = N (337.5-22.49), 2 = NE (22.5-67.49), ...

                            When cond is present, an additional class 0 is added
                            containing events for which cond is false.
                            Input values must be expressed in degree, and are 
                            taken modulo 360.
                            The number of classes is saved internally, in association
                            with the key ``'numberOfClasses'``, and can be stored in
                            any output supporting the auxiliary meta-information
                            ``'localInformationNumber'`` by setting in active dictionary
                            localInfoNumberUse=numberOfClasses.

``'rate'``                      
                            compute field rate in seconds, by dividing the field 
                            value by the length of the time interval for which the 
                            field is representative (only when time range indicator 
                            is one of 2, 3, 4 or 5)

**Operators using location specific information**

``'hshift_loc'``                
                            add location height to the active field; the field to
                            transform must be expressed in meter.
``'hcor,value'``                
                            a field correction of value units per meter is applied,
                            based on the difference between location height and
                            model orography 

The previous operator may only be applied at grid points associated with a location 
and require the model surface height to be available from ``'INCORE'`` storage.
See also voper=``'translate_simple....'`` for a different way to apply a height
correction.

**Operators used to apply statistical post-processing:**

``'kfcor[,pr_1]'``                  
  apply Kalman filter correction
``'kfincr[,pr_1]'``                 
  compute increment to direct model output due to Kalman filter
``'moscor_mlr[,pr_1,...,pr_n]'``    
  estimate parameter, using MOS based on location-dependent multiple linear regression (mlr)
``'moscor_global[,pr_1,...,pr_n]'`` 
  estimate parameter, using MOS based on global multiple linear regression (global)
``'moscor_logreg,<threshold>[,pr_1,...,pr_n]'``    
  estimate probability of threshold exceedance, using MOS based on logistic regression (logreg)
``'moscor_extlogreg,<threshold>[,pr_1,...,pr_n]'`` 
  estimate probability of threshold exceedance, using MOS based on extended logistic regression (extlogreg)

**Index**

============ ========================================================================
============ ========================================================================
n            integer compatible with format I8
loc          location tag
value        real number compatible with format F14.0
cond         logical mask (see 4.3.7)
a0, a1...    real coefficients
f
fmin
fmax
ffix
dfmax
tag
tagi         field tags refering to other fields available in the current
              iteration (either single 2d field, or field matching the 
              properties of the currently processed field)
pr_1
pr_n         field tags refering to other fields available in the current
              iteration and pointing to predictants of a statistical operator
============ ========================================================================


.. note::
  Note about the statistical operators:

  * The operators
      ``'kfcor'``, ``'kfincr'``, ``'moscor_mlr'``, ``'moscor_logreg'``, and ``'moscor_extlogreg'``
    
    may only be applied at grid points associated with a location and require access to 
    Kalman filter and MOS coefficients, respectively, as specified in location_list_additional
    (see 6.3 for more details). The consistency of internal location coordinates with those
    given in the external coefficient files is checked when the respective coefficient files
    are read. In addition, the value of :nl:arg:`kf_model_name` and mos_model_name, respectively, as declared
    in location_list_additional, is checked against the internal model name value. The behaviour
    of the program in case of missing correction data is controlled by the namelist variable
    :nl:arg:`out_mode_kfcor_na` and out_mode_moscor_na, respectively (see 4.3.2).

  * The operator
      ``'moscor_global'``

    may be applied at any grid point.

  * The operators
      ``'moscor_logreg'`` and ``'moscor_extlogreg'``

    estimate the probability of exceeding the threshold specified as first operator argument.
    The product category is thus changed to ``'stat_probability'``, when one of these operators
    is applied. Moreover, these two operators must be combined with the operator
    new_field_id='name,1' in order to set the unit of the resulting probability field to 1
    (GRIB1 conventions on how to code probabilities that have been estimated by a statistical
    model are given in chapter 9).

  * Only Kalman filter and MOS with linear statistical model are supported. All predictor
    fields have to be present in the iteration in which the operator is applied. They are
    defined in the Error_model and Forecast_model, respectively, in the corresponding
    :nl:arg:`location_list_additional` file. If more than one copy of one of the predictor fields exists
    in the same iteration, the field must be uniquely tagged, and the list of unique predictor
    field names and/or tags pr_1 to pr_n must be given as operator argument. The predictor
    fields in the list must occur in the same order as in the Error_model and Forecast_model,
    respectively. If no argument is present, the names of the predictors in the Error_model and
    Forecast_model specification are used to find the predictor fields.

  * To use the MOS operator to create a new field, not available in the input file, as
    opposed to correcting an existing field, the field has first to be created. This is
    done at the start of a new iteration by using the operator use_operator='__CREATE__' 
    (see 4.3.5 below); the MOS operator is then applied in the same iteration to set
    the values of the created field.

  * ``'kfcor'`` and ``'kfincr'`` expect exactly one predictor field coinciding with the field that
    will be corrected, whereas for ``'moscor_mlr'``, ``'moscor_global'``, ``'moscor_logreg'``, and
    ``'moscor_extlogreg'`` more than one predictor field is allowed. 

  * Kalman filter and MOS estimation can only be applied to fields of product categories
    ``'determinist'``, ``'eps_mean'``, ``'eps_quantile'``, and ``'eps_member'``.

  * More details on parameter and probability estimation based on statistical models are
    given in 6.3.

.. note::
  Note about logarithm_e/exponential/inverse/product/quotient/ratio/
  azimut_class/rate/moscor_logreg/moscor_extlogreg operators: 

  * These operators change the field identity, and :nl:arg:`new_field_id` must be set. 

.. note::
  Note about all operators:

  * The result is undefined where some of the operands are undefined
   
 
.. nl:argument:: new_field_id

    :format: <string>              where <string> = name[,units]


    It is possible to re-set the name and the units of the active field with 'new_field_id'.
    The new name must be defined in the active dictionary; if not set, the new units are
    those associated with the new name in the active dictionary.
   
    The level information, the type of statistical processing (time range information),
    and the genrating process type will also be reset when the associated information
    is available in the dictionary, and when this information is compatible with the
    value of the processed record. In particular, when a non trivial type of statistical
    processing is defined in the dictionary (e.g. average), and when the decoded record
    is valid for a single time level, the dictionary value will be used and the time
    range will be set to [0,0] (with respect to the validation date).
   
    When the new field identity is already present in the current processing iteration,
    all fields with the same identity will be merged into a single field, combining
    the fields along the time dimension (an exception is raised when this is not 
    possible).
    
    A side effect of some operators is to change the identity of the field being 
    transformed; in such a case it is mandatory to reset the field name (or to set
    :nl:arg:`strict_usage` to false). 
    This concerns the following operators:

    :nl:arg:`voper`        
      in {``derivate_k``, ``upscale_gradient_min``, ``upscale_gradient_max``}
    :nl:arg:`hoper`        
      in {``regcnt``, ``regcnt%``, ``regfit``, ``tiled_sqcnt%``, ``sqcnt%``, ``diskcnt%``}
    :nl:arg:`poper`        
      in {``product``, ``quotient``, ``ratio``, ``azimut_class``, ``rate``, ``moscor_logreg``, ``moscor_extlogreg``}
    :nl:arg:`compare_fct`  
      in {``ratio``, ``ratio_abs``}
   
    It is possible to reset the field units only, by using the same name as the field 
    being transformed and by defining new units. Note that no internal consistency check
    is currently made on field units; this value is only used when producing some output
    (e.g. NetCDF). Note also that field units may be reset with the :nl:arg:`set_units` operator;
    when used in the last iteration, :nl:arg:`set_units` has precedance over new_field_id.

.. _section 4.3.5:

Generated fields and associated operations
------------------------------------------

All the fields extracted for a specific output file, as described in 4.3.3
and 4.3.4, are available for further processing. This processing is implemented
in multiple iterations; currently up to five iterations are supported. The data
set available at the start of each iteration is limited to the result of the
previous iteration, or to the set of extracted fields in the case of the first
iteration.

Fields can be simply copied from one iteration to the next, or new fields
can be computed. Unlike the rule for the definition of extracted fields,
multiple instances of the same fields may be defined to collect multiple
transformations of the same field in the same output (e.g. T min, max and 
mean).

The first iteration is triggered by setting ``&Process tmp1_field=... /``, the
second one by setting ``&Process tmp2_field=... /``, the third one by setting
``&Process tmp3_field=... /``, the fourth one by ``&Process tmp4_field=... /``, the fifth
one by ``&Process tmp5_field=... /``, and the last one by ``&Process out_field=... /``. 
When no declaration for a specific iteration is present, this iteration is
simply ignored.

All fields specified in the last iteration are written into the output file;
when no iteration is defined, all extracted fields are written. When
INCLUDE_ALL mode is active, all fields not explicitely specified in the
extraction step are also transfered in the output.

The order of fields in the output file depends on the selection mode and
on the declaration or not of additional processing iterations (besides
the in_field iteration):

**Only in_field iteration**
    All modes

    1. follows the order of input files processing

    INCLUDE_ONLY mode

    1. follows the order of namelist appearance
    2. for unspecified field characteristics, follows order from input

    INCLUDE_ALL, EXCLUDE mode

    1. follows order from input

**Additional processing iterations**
    All modes

    1. follows the order of namelist appearance
    2. for unspecified field characteristics, follows order from input

    INCLUDE_ALL mode

    3. additional fields are concatenated at output end

A new field will be computed when it is not found in the previous
iteration, or when 'use_operator=...' is explicitely specified. All
parents of the field must be present in the previous iteration for this
operation to be successful. When one or more of the parents are undefined
for a specific validation date, the derived field will also be undefined 
for this validation date.

The algorithm used to compute a new field can either be explicitely
specified by setting 'use_operator=...', or depend on the field name 
and the product category. In this latter case, either a field with a
new name is computed (e.g. RELHUM from T and TD), or a field belonging
to a new product category but representing the same physical quantity 
is computed (e.g. TOT_PREC probability from TOT_PREC eps members).

The list of operations defined by the name of the field to compute is
available in section 5.1 below.

The list of operations modifying the product category of the field
is:

======================== ==================================
  product category of:
-----------------------------------------------------------
  parent field           new field   
======================== ==================================
  deterministic          neighbourhood probability   
  eps member             eps mean                    
  eps member             eps perturbation                    
  eps member             eps spread 
  eps member             eps standard deviation                    
  eps member             eps quantile                  
  eps member             eps quantile difference
  eps member             eps probability             
======================== ==================================

When not explicitely specified, or not defined in the active dictionary,
the characteristics of the computed field (e.g. product, level ...) are
inherited from its main parent. A unique main parent is associated with
each operator.

For operators modifying the product category, the name of the main parent
is the same as the name of the field to compute, otherwise it is operator
dependent (see 5.1). In this latter case, the name of the main parent
must be declared, either in the active dictionary or as first argument
of 'use_tag'.

Once all fields for the current iteration have been collected or
computed, an optional tag is associated to each one. Each field may
then be transformed by one or more operators, in the order :nl:arg:`hoper`,
scale/offset, :nl:arg:`voper`, :nl:arg:`toper`, poper, :nl:arg:`poper2`, :nl:arg:`poper3`, poper4, :nl:arg:`poper5`,
spatial filter, new_field_id; the order is hardcoded and does not
depend on the way the namelist is defined. Each class of operators 
is applied in turn to all the concerned fields; within a class the
processing order is arbitrary. 

Before creating the final product, a last set of global operations
may be applied, in the order n to m operator (out_postproc_module),
re-gridding (out_regrid_target, :nl:arg:`out_regrid_method` ...), re-setting
meta-information (originating_center, :nl:arg:`production_status`, genproc_type, 
auxiliary_metainfo, :nl:arg:`out_model_name`, :nl:arg:`out_genproc_type`,
out_product_category, :nl:arg:`out_vertical_coordinate`,
:nl:arg:`out_auxiliary_metainfo`, set_*), filtering data set 
(out_filter_cst, out_filter_time).
Unlike the operators described in this section, with the exception 
of the set_* operators, these transformations apply on all available
fields (in particular on passive fields when INCLUDE_ALL mode is set).

The limitation due to the pre-defined order of transformations within
a single iteration can be circumvented by implementing the desired
transformations with multiple iterations.
    
**First iteration:**

.. nl:argument:: tmp1_field

    :format: <name or tag of field to transfer> OR <name of field to compute>


    The specified field is either computed, when not available in the previous
    iteration or when :nl:arg:`use_operator` is set, or transfered from the previous
    iteration.

.. nl:argument:: use_tag

    :format: <tag1, tag2, ... (string)>

    When ambiguous, the set of parents used to derive a specific field can be
    restricted to the list defined by use_tag; this is required when multiple
    instances of the same field are available in the previous iteration, or
    when multiple sets of parent fields are supported by the associated operator.
   
    The :nl:arg:`use_tag` list refers to tags of fields within the previous iteration.
    If present, this list must be exhaustive, and its first element defines 
    the operator main parent. This is the only way to filter the list of parent 
    fields.
   
    Besides the possibility to filter the list of parents, :nl:arg:`use_tag` may also
    be used to re-define the main parent of the field to be computed; the
    first element of :nl:arg:`use_tag` overrides any main parent set in the active
    dictionary (but information on operator halo remains the one derived
    from dictonary). 
   
    The :nl:arg:`use_tag` list will also filter the set of fields to transfer from 
    one iteration to the next, when no new field is computed.
      
.. nl:argument:: use_operator

    :format: <name of operator (string)>


    It is also possible to explicitely select the operator used to compute a 
    new field. In this case, :nl:arg:`use_operator` must be associated with use_tag,
    and the main parent of the derived field is defined to be the first
    element of use_tag. The name of the operator must match the name of a
    procedure available in one of the module :nl:arg:`fxtr_operator_generic` or
    :nl:arg:`fxtr_operator_specific` (see section 5.2 of this file for the list of
    available operators; see also README.user.locale).
    
    By specifying :nl:arg:`use_operator` one forces the associated field to be
    computed, even when readily available in the previous iteration.
    
    A special operator, '__CREATE\__', is available to create a new field by
    copying all meta-information of the main parent specified by use_tag.
    The field to create must be defined in the active dictionary; all values
    of the field are set to undefined. When used in conjunction with 'moscor_mlr',
    this feature supports a MOS forecast by deriving a new field from a set
    of predictors (see :nl:arg:`moscor_mlr`).

Levels
^^^^^^

.. nl:argument:: level_class
    :default: 'all'

    :format: <type of level to consider>
    :noindex:

    The type of levels to consider may be explicitely specified; see :nl:arg:`level_class`
    for more details.

**Levels, can be defined by ...**

* a list of values

  .. nl:argument:: levlist

      :format: <level>,<level>,...    
      :noindex:
      
      -1 means all available levels

* specifying start value, end value and increment

  .. nl:argument:: levmin

      :format: <min value>
      :noindex:

  .. nl:argument:: levmax

      :format: <max value>
      :noindex:

  .. nl:argument:: levinc

      :format: <increment>
      :noindex:

The associated physical units may be specified by ::

  level_units = <units>  (optional)

see :nl:arg:`levlist` for a full description.
   
.. note:: for fields defined on layers, the values specified by levlist or
          levmin ... always refer to the top surface of the layer (e.g. the
          lowest layer of a field defined on hybrid model levels, with model
          level indices increasing with height, is selected by setting levlist=2)

**EPS members, can be defined by ...**

* a list of values

  .. nl:argument:: epslist

      :format: <member>,<member>,...   
      :noindex:
      
      -1 means all available members

* specifying start value, end value and increment

  .. nl:argument:: epsstart

      :format: <start eps member>
      :noindex:

  .. nl:argument:: epsstop

      :format: <stop eps member>
      :noindex:

  .. nl:argument:: epsincr
      :default: 1

      :format: <increment eps member>
      :noindex:

Product Category
^^^^^^^^^^^^^^^^

.. nl:argument:: product_category
    :default: 'all'

    :format: <product category>


    The product category may be explicitely specified; supported values are:
      | 'deterministic',
      | 'eps_control', 'eps_member', 'eps_mean', 'eps_perturbation',
      | 'eps_spread',
      | 'eps_standard_deviation', 'eps_standard_deviation_normed',
      | 'eps_quantile', 'eps_quantile_difference', 
      | 'eps_probability', 'nbh_probability', 'stat_probability',
      | 'eps_probability_refdist',
      | 'eps_extreme_forecast_index', 'eps_shift_of_tail_index',
      | 'satellite', 'radar', 
      | 'all'                              
   
    It is necessary to explicitely specify the product category to trigger 
    the transformation of some fields from one category to another (e.g. to
    compute EPS probability from EPS member). In this case additional
    information may be required:

    .. flat-table::
        :header-rows: 1

        * - parent field
          - new field
          - required information

        * - eps_member
          - eps_mean
          - :nl:arg:`out_mode_weighted_member`

        * - eps_member
          - eps_perturbation
          - :nl:arg:`out_mode_weighted_member`

        * - eps_member
          - eps_standard_deviation
          - :nl:arg:`out_mode_weighted_member`

        * - eps_member
          - eps_standard_deviation_normed
          - :nl:arg:`out_mode_weighted_member`
            

        * - eps_member
          - eps_quantile
          - | :nl:arg:`quantile` (one value)
            | :nl:arg:`out_mode_weighted_member`

        * - eps_member
          - eps_quantile_difference
          - | :nl:arg:`quantile` (2 values)
            | :nl:arg:`out_mode_weighted_member`

        * - eps_member
          - eps_probability
          - | :nl:arg:`prob_interval` or
            | :nl:arg:`prob_tlow` or/and :nl:arg:`prob_thigh`
            | :nl:arg:`out_mode_weighted_member`

        * - deterministic
          - neighbourhood_probability
          - | :nl:arg:`prob_interval` or
            | :nl:arg:`prob_tlow` or/and :nl:arg:`prob_thigh`
   
    Some remarks on the way these quantities are computed:

    eps perturbation       :  
      difference between member and eps mean
    eps spread             :  
      computed as "eps standard deviation"
      (with out_mode_weighted_member=.false.)
    eps standard deviation :  
      corrected sample standard deviation 
    eps standard deviation normed : 
      normalized with respect to EPS mean, set to undefined when the EPS mean is too close to 0

    .. note:: 
      other PDF characteristics can also be computed by using special
      quantile values:

      - the minimum of the ensemble by setting the quantile to 0.
      - the maximum of the ensemble by setting the quantile to  100.
      - the median of the ensemble by setting the quantile to  50.

PDF Characteristics
^^^^^^^^^^^^^^^^^^^

Characteristics of PDF related fields are specified by:

* for probability

  .. nl:argument:: prob_interval
      :noindex:

      :format: <[x,y], [x,y[, ]x,y] or ]x,y[, as string>

      interval of values, expressed in field units

      with  x and y being either real numbers, or
      ``'-infinite'`` or ``'infinite'``

* for probability, depreciated

  interval lower and upper boundaries, expressed in field units, the boundaries are *not* included in the interval
    
  .. nl:argument:: prob_tlow
      :noindex:
      :default: -infinity

      :format: <interval lower threshold (real)>

  .. nl:argument:: prob_thigh
      :noindex:

      :format: <interval higher threshold> (real, default is plus infinite)

* for quantile
  
  .. nl:argument:: quantile
      :noindex:

      :format: <quantile, real>

      a value for a quantile, betwen 0 and 100

OR

* for difference of quantiles

  .. nl:argument:: quantile
      :noindex:

      :format: <quantile1,real>, <quantile2,real>

      two quantile values, both between 0 and 100

* for probability with respect to a reference distribution

  .. nl:argument:: ref_quantile
      :noindex:

      :format: <order of reference quantile, integer> [,<number of reference quantile, integer>]

      the order of the quantile in the reference distribution and
      the total number of quantiles used to represent the reference
      distribution

* for shift of tail index

  .. nl:argument:: efi_order
      :noindex:

      :format: <percentile fcst, integer>[,<percentile climate, integer>]


.. nl:argument:: tag
    :noindex:

    :format: <tag associated with field (string, <= 24 characters)>


Transformations
^^^^^^^^^^^^^^^

Available transformations are ...

.. nl:argument:: hoper
    :noindex:

    :format: <horizontal operator> 

.. nl:argument:: hoper_mask
    :noindex:

    :format: <associated logical condition>

.. nl:argument:: scale
    :noindex:

    :format: <scaling factor> 

.. nl:argument:: offset
    :noindex:

    :format: <offset factor> 

.. nl:argument:: voper
    :noindex:

    :format: <vertical operator>

.. nl:argument:: voper_lev
    :noindex:

    :format: <level>,<level>,...  (target levels for vertical operator)

.. nl:argument:: voper_lev_units
    :noindex:

    :format: <units>              (associated physical units, optional; see level_units)

.. nl:argument:: voper_use_tag
    :noindex:

    :format: <tag1, tag2, ...>    (optional; tag of auxiliary fields)

.. nl:argument:: toper
    :noindex:

    :format: <time operator>     

.. nl:argument:: toper_mask
    :noindex:

    :format: <associated logical condition>

.. nl:argument:: poper
    :noindex:

    :format: <point operator> 

.. nl:argument:: poper2
    :noindex:

    :format: <point operator> 

.. nl:argument:: poper3
    :noindex:

    :format: <point operator> 

.. nl:argument:: poper4
    :noindex:

    :format: <point operator> 

.. nl:argument:: poper5
    :noindex:

    :format: <point operator> 

.. nl:argument:: new_field_id
    :noindex:

    :format: <string>              where <string> = name[,units]

and also, but only if it is the last processing iteration

.. nl:argument:: set_units
    :noindex:

    :format: <string>

.. nl:argument:: set_reference_date
    :noindex:

    :format: <integer, yyyymmddhhmm>

.. nl:argument:: set_trange_type
    :noindex:

    :format: <string>

.. nl:argument:: set_product_category
    :noindex:

    :format: <string>

.. nl:argument:: set_epsmember_identity
    :noindex:

    :format: <member value>[,<number of members>]

.. nl:argument:: set_genproc_type
    :noindex:

    :format: <string>

.. nl:argument:: set_auxiliary_metainfo
    :noindex:

    :format: <sring>[,<string> ...] (each string expressed as key=value)


.. note:: 
  Note about the usage of :nl:arg:`hoper` in one of the generate field iteration:
  data reduction from full field to the user defined subset is only done
  after the last horizontal transformation has been applied. This has as
  consequences:

  1.  to optimize memory and performance, it is advised to apply this
      type of transformation as soon as possible in the generation process.

Further Iterations
^^^^^^^^^^^^^^^^^^

**Second iteration:**
  .. nl:argument:: tmp2_field

      :format: <name or tag of field to transfer> OR <name of field to compute>

      The same set of operators are available as for the second iteration
      (see above).

**Third iteration:**
  .. nl:argument:: tmp3_field

      :format: <name or tag of field to transfer> OR <name of field to compute>

      The same set of operators are available as for the second iteration
      (see above).

**Fourth iteration:**
  .. nl:argument:: tmp4_field

      :format: <name or tag of field to transfer> OR <name of field to compute>

      The same set of operators are available as for the second iteration
      (see above).

**Fifth iteration:**
  .. nl:argument:: tmp5_field

      :format: <name or tag of field to transfer> OR <name of field to compute>

      The same set of operators are available as for the second iteration
      (see above).

**Sixth iteration:**
  .. nl:argument:: out_field

      :format: <name or tag of field to transfer> OR <name of field to compute>

      The same set of operators are available as for the second iteration
      (see above).

 .. _section 4.3.6:

Output related field characteristics
------------------------------------

Some field related output characteristics can be defined in the last processing
iteration (incl. the extract iteration when no processing iteration is defined).
These settings always override any global setting defined for the concerned 
output. Most of these operations are limited to some types of output (see
:ref:`chapter 7` for the details).

.. nl:argument:: set_undefcode

    :format: <missing value code, real>

    code used to mark undefined values, override value of :nl:arg:`out_type_undefcode`. 

    One must be cautious when using set_undefcode, because this value is not declared
    explicitely in the output file, and may differ from the global undef code used
    for the concerned output file.

.. nl:argument:: set_packing

    :format: <packing method, string>

    packing algorithm (see :nl:arg:`out_type_packing` for a description). 
    When set, override out_type_packing.

.. nl:argument:: set_description

    :format: <string>

    set the field description (otherwise this value is taken from the active dictionary).
    This information is used at the following places:
    
    * long name in NETCDF output (e.g. used for plot labelling),
    * part of description string in table of content associated 
      with GRIB1, GRIB2, and NETCDF output.
    * field description in ASCII_TABLE extra header 

.. nl:argument:: set_comment

    :format: <string>

    associate a comment with the current field; a comment is any freely definable string, and is used at the following place:
    
    * field attribute 'comment' in NETCDF output.

.. nl:argument:: use_alternate_code

    :format: <logical>

    when set to true, alternate GRIB 1 code found in active
    dictionary will be used; alternate code is defined in the
    dictionary by setting the attributes 'alternateTable' and
    'alternateParameter'. When no alternate code is defined,
    the usual code is applied.
    The same effect can be obtained by setting to true the
    key :nl:arg:`out_type_alternate_code`, but for all fields of the
    concerned output.
 
.. _section 4.3.7:

Masks
-----

Some operators support the specification of an auxiliary field representing
a logical condition, a so-called 'mask'.

A mask is specified by:

* ``token && token && ...``    (logical AND)       

or    

* ``token || token || ...``    (logical OR)

``token`` is one of :
  | ``tag<value``, ``tag<=value``, ``tag=value``, ``tag>value``, ``tag>=value``, ``tag!value``,
  | ``<value``,    ``<=value``,    ``=value``,    ``>value``,    ``>=value``,    ``!value``

* '!' stands for the comparison operator 'not equal'
* tag is the tag or the name of a field available in
  the current iteration, which is used to construct
  the logical condition;
* the condition refers to the current field when tag
  is missing;
* by convention the condition is evaluated to false
  where the field is undefined;
* fields on staggered grid are also supported (in this
  case, grid points on base grid and on staggered grid
  are associated).

The field used to evaluate a specific condition must either be fully
specified by the value of tag, or by the value of tag and the
characteristics of the currently processed field (level characteristics
and/or eps characteristics).